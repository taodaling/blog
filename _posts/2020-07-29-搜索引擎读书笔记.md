---
categories: note
layout: post
---

- Table
{:toc}

# 实战

## 项目搭建

创建一个maven项目，加入LUCENE的maven依赖。

```xml
<!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-core -->
<dependency>
    <groupId>org.apache.lucene</groupId>
    <artifactId>lucene-core</artifactId>
    <version>8.0.0</version>
</dependency>
```

之后加入[开源的IK分词器](https://github.com/magese/ik-analyzer-solr)。

```xml
<!-- Maven仓库地址 -->
<dependency>
    <groupId>com.github.magese</groupId>
    <artifactId>ik-analyzer</artifactId>
    <version>8.3.0</version>
</dependency>
```

## LUCENE分词

LUCENE中，分词主要依靠Analyzer类解析实现。下面是实战内容：

```java
public class StdAnalyzer {
    public static void main(String[] args) throws IOException {
        BufferedReader br = new BufferedReader(new InputStreamReader(System.in));
        while (true) {
            String line = br.readLine();
            Analyzer analyzer = new IKAnalyzer();
            TokenStream ts = analyzer.tokenStream(line, line);
            ts.reset();
            CharTermAttribute attr = ts.getAttribute(CharTermAttribute.class);
            List<String> list = new ArrayList<String>();
            while (ts.incrementToken()) {
                list.add(attr.toString());
            }
            System.out.println(list.stream().collect(Collectors.joining("|")));
        }
    }
}
```

要扩展词典，可以修改IK分词器下的`IKAnalyzer.cfg.xml`文件。

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
	<!-- 配置是否加载默认词典 -->
	<entry key="use_main_dict">true</entry>
    <!-- 配置自己的扩展字典，多个用分号分隔 -->
    <entry key="ext_dict">ext.dic;</entry>
    <!-- 配置自己的扩展停止词字典，多个用分号分隔 -->
    <entry key="ext_stopwords">stopword.dic;</entry>
</properties>
```

可以在里面加入新的词典名称，通过`;`分隔即可。

默认的词库在`/resources/dict`目录下面。

比如我们在`/resources`下创建一个名字为`ext2.dic`的文件，里面的内容为

```
厉害了我的哥
```

之后我们修改`IKAnalyzer.cfg.xml`文件。

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
	<!-- 配置是否加载默认词典 -->
	<entry key="use_main_dict">true</entry>
    <!-- 配置自己的扩展字典，多个用分号分隔 -->
    <entry key="ext_dict">ext.dic;ext2.dic;</entry>
    <!-- 配置自己的扩展停止词字典，多个用分号分隔 -->
    <entry key="ext_stopwords">stopword.dic;</entry>
</properties>
```

这样我们新加入的词`厉害了我的哥`就生效了。

## 索引

文档是Lucene索引和搜索的基本单位，比文档更小的单位是字段，字段是文档的一部分，每个字段由三部分组成：名称、类型和取值。一个文档可以有多个字段。

我们先建立一个实体类，代表文档，这里使用lombok简化生成getter、setter等。

```java
@Data
@Builder
public class News {
    private int id;
    private String title;
    private String content;
    private int reply;
}
```

下面演示通过文档建立索引

```java
public class CreateIndex {
    public static void main(String[] args) throws IOException {
        List<News> newsList = Arrays.asList(
                News.builder().id(1).title("早睡早起竟导致脱发").content("某位小学生，遵循父母早睡早起的要求，在10岁的年纪开始脱发")
                        .reply(10).build(),
                News.builder().id(2).title("好好学习导致单身").content("某位年轻人，从小到大好好学习，最后48岁依旧单身，孤独终老")
                        .reply(100).build(),
                News.builder().id(3).title("每日吃饭竟导致胃癌").content("某位中年人，每日吃大米饭，在21岁染上胃癌")
                        .reply(1000).build()
        );


        Analyzer analyzer = new IKAnalyzer();

        IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer);
        iwConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE);

        File indexdir = new File("indexdir");
        indexdir.mkdirs();
        Directory dir = FSDirectory.open(indexdir.toPath());
        IndexWriter writer = new IndexWriter(dir, iwConfig);

        FieldType idType = new FieldType();
        idType.setIndexOptions(IndexOptions.DOCS);
        idType.setStored(true);

        FieldType titleType = new FieldType();
        titleType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
        titleType.setStored(true);
        titleType.setTokenized(true);

        FieldType contentType = new FieldType();
        contentType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
        contentType.setStored(true);
        contentType.setTokenized(true);
        contentType.setStoreTermVectors(true);
        contentType.setStoreTermVectorPositions(true);
        contentType.setStoreTermVectorOffsets(true);
        contentType.setStoreTermVectorPayloads(true);

        for (News news : newsList) {
            Document doc = new Document();
            doc.add(new Field("id", "" + news.getId(), idType));
            doc.add(new Field("title", news.getTitle(), titleType));
            doc.add(new Field("content", news.getContent(), contentType));
            doc.add(new IntPoint("reply", news.getReply()));
            doc.add(new StoredField("reply_display", news.getReply()));
            writer.addDocument(doc);
        }

        writer.commit();
        writer.close();
        dir.close();
    }
}
```

执行后，会在代码根目录下的`indexdir`中创建下面若干文件：

```sh
2020/07/30  15:51               154 segments_1
2020/07/30  15:51                 0 write.lock
2020/07/30  15:51               510 _0.cfe
2020/07/30  15:51             3,768 _0.cfs
2020/07/30  15:51               376 _0.si
```

要查看索引中的数据，可以通过[开源项目luke](https://github.com/DmitryKey/luke)。在release页面找到最新版本的luke发行版。下载即可。下载完成后目录下有`luke.bat`和`luke.sh`两个文件，执行其中任意一个。

下面展示如何删除索引，用的也是IndexWriter，下面我们删除id字段为1的文档。

```java
        Analyzer analyzer = new IKAnalyzer();

        IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer);
        iwConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND);

        indexdir.mkdirs();
        Directory dir = FSDirectory.open(indexdir.toPath());
        IndexWriter writer = new IndexWriter(dir, iwConfig);

        writer.deleteDocuments(new Term("id", "1"));
```

索引的更新实际上是删除旧索引插入新索引的过程。

```java
        News news = News.builder().id(2).title("好好学习导致妻妾成群").content("某位年轻人，从小到大好好学习，最后妻妾成群，28岁应纵欲过度而夭折")
                .reply(100).build();
        Document doc = new Document();
        doc.add(new Field("id", "" + news.getId(), idType));
        doc.add(new Field("title", news.getTitle(), titleType));
        doc.add(new Field("content", news.getContent(), contentType));
        doc.add(new IntPoint("reply", news.getReply()));
        doc.add(new StoredField("reply_display", news.getReply()));
        writer.updateDocument(new Term("id", "2"), doc);
```

## 搜索

### 单字段搜索

我们可以通过构建Query对象，并将Query对象提交给IndexSearcher来完成检索。

```java
        IndexSearcher searcher = new IndexSearcher(reader);
        QueryParser qp = new QueryParser("title", getAnalyzer());
        qp.setDefaultOperator(QueryParser.Operator.AND);
        Query query = qp.parse("好好学习");
        TopDocs topDocs = searcher.search(query, 10);
        for (ScoreDoc doc : topDocs.scoreDocs) {
            System.out.println("summary: " + doc);
            System.out.println("data: " + searcher.doc(doc.doc));
        }
```

里面的QueryParser对象会解析输入的文本，并进行分词。比如好好学习，可能就被解析为`好|好|学习`，任意一个包含其中一个词条的文档都会被匹配（而不是包含整段文本）。如果要求匹配所有的词项，则可以将默认的连接符设置为`AND`。

### 多字段搜索

上面的是仅根据一个字段进行查询。lucene也支持多个字段一起用于查询。需要用到`MultiFieldQueryParser`。

```java
        IndexSearcher searcher = new IndexSearcher(reader);
        QueryParser qp = new MultiFieldQueryParser(new String[]{"title", "content"}, getAnalyzer());
        qp.setDefaultOperator(QueryParser.Operator.AND);
        Query query = qp.parse("夭折");
        TopDocs topDocs = searcher.search(query, 10);
        for (ScoreDoc doc : topDocs.scoreDocs) {
            System.out.println("summary: " + doc);
            System.out.println("data: " + searcher.doc(doc.doc));
        }
```

### 词条搜索

我们也可以不使用Parser，因为Parser会做分词操作，我们可以直接指定词条。

```java
        IndexSearcher searcher = new IndexSearcher(reader);
        Query query = new TermQuery(new Term("content", "夭折"));
        TopDocs topDocs = searcher.search(query, 10);
        for (ScoreDoc doc : topDocs.scoreDocs) {
            System.out.println("summary: " + doc);
            System.out.println("data: " + searcher.doc(doc.doc));
        }
```

### 布尔搜索

布尔搜索，允许我们将多个查询条件通过逻辑运算组合在一起。

```java
        IndexSearcher searcher = new IndexSearcher(reader);
        Query query1 = new TermQuery(new Term("content", "某位"));
        Query query2 = new TermQuery(new Term("id", "3"));
        BooleanQuery bq = new BooleanQuery.Builder()
                .add(query1, BooleanClause.Occur.MUST).add(query2, BooleanClause.Occur.MUST_NOT)
                .build();
        TopDocs topDocs = searcher.search(bq, 10);
        for (ScoreDoc doc : topDocs.scoreDocs) {
            System.out.println("summary: " + doc);
            System.out.println("data: " + searcher.doc(doc.doc));
        }
```

### 范围搜索

Lucene也提供范围查询，比如查询某段时间段内的所有文档。

```java
        IndexSearcher searcher = new IndexSearcher(reader);
        Query query = IntPoint.newRangeQuery("reply", 100, 500);
        TopDocs topDocs = searcher.search(query, 10);
        for (ScoreDoc doc : topDocs.scoreDocs) {
            System.out.println("summary: " + doc);
            System.out.println("data: " + searcher.doc(doc.doc));
        }
```

### 前缀搜索

如果你仅希望查询包含至少一个这样的词条的文档，这个词条以查询词条作为前缀，则可以使用PrefixQuery。

```java
       IndexSearcher searcher = new IndexSearcher(reader);
        Query query = new PrefixQuery(new Term("title", "导"));
        TopDocs topDocs = searcher.search(query, 10);
        for (ScoreDoc doc : topDocs.scoreDocs) {
            System.out.println("summary: " + doc);
            System.out.println("data: " + searcher.doc(doc.doc));
        }
```

### 多关键字搜索

有时候你会需要查询同时包含了多个关键字的文档，这些关键字可能紧密连接，也可能中间穿插无关内容，它们组成一个短语。

PhraseQuery就提供了这个能力，你可以设置slop属性来设置关键字之间最多能允许多少个无关词汇的存在。你可以通过position属性设置每个词条在短语中的相对位置。

```java
        IndexSearcher searcher = new IndexSearcher(reader);
        Query query = new PhraseQuery.Builder()
                .add(new Term("title", "中国"), 1)
                .add(new Term("title", "美国"), 2)
                .add(new Term("title", "日本"), 3)
                .build();
        TopDocs topDocs = searcher.search(query, 10);
        for (ScoreDoc doc : topDocs.scoreDocs) {
            System.out.println("summary: " + doc);
            System.out.println("data: " + searcher.doc(doc.doc));
        }
```