---
categories: tool
layout: post
---



{:toc}

# Concepts

## Docker

Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers.

## Images and containers

A container is launched by running an image. An image is an executable package that includes everything needed to run an application - the code, a runtime, libraries, environment variables and configuration files.

A container is a runtime instance of an image.

## Containers and virtual machines

A container runs natively on Linux and shares the kernel of the host machine with other containers. It runs a discrete process, taking no more memory than any other executable, making it lightweight.

By constrast, a virtual machine runs a full-blown guest operating system with virtual access to host resources through a hypervisor. In general, VMs provide an environment with more resources than most application need.

## Docker layers

1. **Stack** : defining the interactions of all the services
2. **Services** : defining how containers behave in production.
3. **Container** : the bottom of the hierarchy of such an app.

# Command

## image

```
docker image ls
```

List all image

## container

```sh
docker container ls [--all] [-q]
```

List all container running.

- -a: Show all containers (default shows just running).
- -q: Only display numeric IDs

## build

```sh
docker build [-t name] dir
```

build image by dir/Dockerfile with name

## run

```sh
docker run [-d] [-e] [-p localport:virtualport] <imagename>
```

start image

- **-p** : mapping virtualport into localport
- **-d** : run in detach mode
- **-e** : run with extra environment

## stop

```sh
docker container stop <containerid>
```

Stop container by id.

## login

```sh
docker login
```

Log in to the Docker registry on your local machine.

## tag

```sh
docker tag image username/repository:tag
```

add a new tag for image.

## push

```sh
docker push username/repository:tag
```

Push your local tag to the remote repository which you have logged in.

## swarm

```sh
docker swarm init
```

Initialize a swarm.

```sh
docker swarm leave [--force]
```

Leave the swarm.

- -f: Force this node to leave the swarm, ignoring warnings.

## stack

```sh
docker stack deploy -c [composer-file] <service-name>
```

Deploy a new stack or update an existing stack.

```sh
docker stack ls
```

List stacks.

```sh
docker stack rm STACK [STACK...]
```

Remove one or more stacks.

## service

```sh
docker service ps <service-name>
```

List the tasks of one or more services.

## node

```sh
docker node ls
```

List nodes in the swarm.

### logs

```sh
docker logs [-f] [-t] <containerId>
```

Fetch the logs of a container.

- -f: Follow log output
- -t: SHow timestamps

### volume

```sh
docker volume create <name>
```

create a new volume.

```sh
docker volume inspect <name>
```

display detailed information on one or more volumes.



# Docker

## Dockerfile

These portable images are defined by something called a `Dockerfile`.

Dockerfile defines what goes on in the environment inside your container. Access to resources like networking interfaces and disk drives is virtualized inside this environment, which is isolated from the rest of your system, so you need to map ports to the outside world, and the specified about what files you want to copy in to that environment. 

However, after doing that, you can expect that the build of your app defined in this dockerfile behaves exactly the same wherever it runs.

```dockerfile
# Use an official Python runtime as a parent image
FROM python:2.7-slim

# Set the working directory to /app
WORKDIR /app

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed packages specified in requirements.txt
RUN pip install --trusted-host pypi.python.org -r requirements.txt

# Make port 80 available to the world outside this container
EXPOSE 80

# mount /data of internal file system of container, it means your service could modify it, write some data
# you might use -v localDirectory when invoke docker run to mount the virutal diretory to real directory
VOLUME /data 

# Define environment variable
ENV NAME World

# Run app.py when the container launches
CMD ["python", "app.py"]
```

## Share your image

A registry is a collection of  repositories, and a repository is a collection of images. An account on a registry can create many repositories. The docker CLI uses Docker's public registry by default. Use `Docker login` to log in the docker.

The notation for associating a local image with a repository on a registry is `username/repository:tag`. The tag is optional, but recommended, since it is the mechanism that registries use to give Docker images a version. Give the repository and tag meaningful names for the context, such as get-started:part2. This will put the image in the get-started repository and tag it as part2.Use `tag` command to associate a tag with image.

Use `push` command to push your tag to remote registry and after that, you can call `docker run` anywhere. If the image isn't available locally on the machine, Docker will pull it from the repository.

## docker-compose.yml

A `docker-compose.yml` file is a YAML file that defines how Docker containers should behave in production.

```yaml
version: "3"
services:
  web:
    image: username/repo:tag #pull the image
    deploy:
      replicas: 5 #run 5 instances of that images as a service
      resources:
        limits:
          cpus: "0.1" #each instance use no more than 10% of the cpu
          memory: 50M #each instance use no more than 50MB of RAM
      restart_policy:
        condition: on-failure #Immediately restart containers if one fails
    ports:
      - "80:80" #map local port 80 to the container port 80
    networks:
      - webnet #Instruct web's containers to share port 80 via a load-balanced network called webnet.
networks:
  webnet: #Define the webnet network with the defaul settingsa(which is a load-balanced overlay network)
```

A single container running in a service is called a task. Tasks are given unique IDs that numerically increment, up to the number of `replicas` your defined in `docker-compose.yml`.

## Scale the app

You can scale the app by changing the replicas value in docker-compose.yml, saving the change and re-running the `docker stack deploy` command.Docker will do an in-place update, no need to tear the stack down first or kill any containers.

## Swarm

A swarm is a group of machine that are running Docker and joined into a cluster. After that has happened, you continue to run the Docker commands you're used to,  but now they are executed on a cluster by a swarm manager. The machines in a swarm can be physical or virtual. After joining a swarm, they are referred to as node.

Swarm managers can use serveral strategies to run containers, such as "emptiest node" -which fills the least utilized machines with containers. Or "global", which ensures that each machine gets exactly one instance of the specified container. You instruct the swarm manager to use these strategies in the Compose file, just like the one you have already been using.

Swarm managers are the only machines in a swarm that can execute your commands, or authorize other machines to join the swarm as workers. Workers are just there to provide capacity and do not have the authority to tell any other machine what it can and cannot do.

### Set up your swarm

A swarm is made up of multiple nodes, which can be either physical or virtual machines. The basic concept is simple enough: `docker swarm init` to enable swarm mode and make your current machine to have them join the swarm as workers. Run `docker swarm join` on other machines to have them join the swarm as workers.

The response to `docker swarm init` contains a pre-configured `docker swarm join` command for you to run on any nodes you want to add.

### Accessing your cluster

You can access your app from all the ips joined in swarm include manage. The network you created is shared between them and load-balancing. The reason both IP addresses work is that nodes in a swarm participate in an ingress routing mesh. This ensures that a service deployed at a certain port within your swarm always has that port reserved to itself, no matter what node is actually running the container.

## Stack

A stack is a group of interrelated services that share dependencies, and can be orchestrated and scaled together. A single stack is capable of defining and coordinating the functionality of an entire application.

```yaml
version: "3"
services:
  web:
    image: username/repo:tag
    deploy:
      replicas: 5
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: "0.1"
          memory: 50M
    ports:
      - "80:80"
    networks:
      - webnet
  visualizer:
    image: dockersamples/visualizer:stable
    ports:
      - "8080:8080"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock" 
    deploy:
      placement:
        constraints: [node.role == manager]
    networks:
      - webnet
  redis:
    image: redis
    ports:
      - "6379:6379"
    volumes:
      - /home/docker/data:/data #map the local file system /home/docker/data to the inner file system of container /data, the inner file system will get wiped out if the container were ever redeployed. You should create the local file system directory in advance
    deploy:
      placement:
        constraints: [node.role == manager] #only deploy this service in manager node of swarm
    command: redis-server --appendonly yes
    networks:
      - webnet
networks:
  webnet:
```

# Portainer

Portainer is built to run on Docker and is really simple to deploy.

## Deployment

Run as a simple container

```sh
docker volume create portainer_data

docker run -d -p 9000:9000 --name portainer --restart always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer
```

Inside a swarm cluster

```sh
curl -L https://portainer.io/download/portainer-agent-stack.yml -o portainer-agent-stack.yml

docker stack deploy --compose-file=portainer-agent-stack.yml portainer
```

