---
categories: notebook
layout: post
---

- Table
{:toc}

# 基本概念

深度学习是机器学习的一个分支（子集），它通过建立多个层次进行学习，随着层次递进，信息被不断提纯。其中层次的数目也称为模型的深度。现代深度学习往往包含数十上百级别的层次，这些层次都是从训练数据中通过名为神经网络的模型学习得到的。而与深度学习不同，其它的机器学习的重点往往仅仅是学习一两层的数据表示，因此也被称作浅层学习。

深度学习仅仅是灵感来自与大脑，但是在机制上并没有证据能证明它们之间有任何相似性。深度学习仅仅是一种数学框架而已。

神经网络中每层对输入的变换实际上是由一组权重参数决定的，而学习的实际目的是为每一层找到一组合适的参数。但是由于神经网络中有着数以万记的参数，并且这些参数之间是共同作用得到结果的，因此同时找到所有参数的合适值是一个艰巨的任务。

损失值用于衡量神经网络预测结果与实际结果的距离。一开始的时候我们为所有神经网络参数赋予随机值，之后我们根据通过反向传播算法来调整参数，从而降低损失值。

相较于SVM，随机森林等之前流行的浅层学习，深度学习不仅拥有更好的预测准确度，并且能够自动完成特征工程的工作。

那么是否可以通过叠加多个浅层学习达到和深度学习一样的效果呢。事实上这是不行的，在叠加多个浅层学习的情况下，每一层仅仅为当前层的精确度服务，得到的结果无法达到最优。而深度学习所有层的参数都是为了同一个的目标努力，因此才能同时达到最优。

# 数学概念

## 张量

张量（tensor），在编程中对应的就是多维数组，我们可以将其理解为多个维度的向量。比如标量是零维的张量，向量就是一维的张量，而矩阵则是二维的张量。张量的每个维度通常称作轴。

在python中可以通过`x.ndim`来获得某个张量的维度。通过`x.shape`可以获得张量沿着各个轴的维度。

轴数较少的张量可以通过广播的方式自动提升轴数，比如形状为`(3, 2)`的矩阵$A$和形状为$(2)$的向量$b$，$b$可以提升为形状为`(3,2)`的矩阵$B$，其中对于$1\leq i\leq 3$有$B_i=b$。（广播的前提是轴数较少者形状是轴数较多者形状的一个后缀）。

对于张量运算，大部分运算都是逐元素的，比如加减法等。

# 算法

## 随机梯度下降（SGD，stochastic gradient descent）

神经网络的训练，一般需要使用梯度下降的技术来寻找最小值。其原理是通过反向传播算法计算出损失函数的梯度（与神经网络参数是同样形状的张量），之后将神经网络的参数沿着梯度的反方向移动一小步。

随机梯度下降是结合了随机采样的梯度下降。即我们每次计算梯度的时候，不会考虑所有的样本，而是仅仅着眼于一个随机选取的小批次样本。

SGD还有其他一些变体，这些变体被称作优化器（optimizer）。其中动量法很值得关注，它同时解决了收敛速度和局部极值点的问题，可以理解为在曲面中投入一个小球，小球在曲面中自如滚动，最后停下的点就是找到的最小值点。

## 数据标准化

输入的特征的范围不同，会导致训练模型变得困难。一种简单的方式是将特征全部进行标准化，对每个特征独立执行下面过程：

- 计算训练集中该特征的均值$\mu$和标准差$\sigma$
- 将特征值$x$转换为$\frac{x-\mu}{\sigma}$

经过这个流程，每个特征的均值均为$0$，且标准差为$1$。

代码如下：

```python
def normalize(x):
    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)
```

## K则验证

当训练数据不足的时候，再切分出验证集，可能导致验证分数有很大的方差（波动很大）。一种解决方法就是K则验证，其原理是将训练集等分为$k$部分，之后对每个部分$i$，以其他$k-1$个部分作为训练集，而将部分$i$作为验证集，训练一个单独的模型。这样我们可以得到$k$个模型，选择其中最优的即可。这里$k$一般取$4,5$。

# 评估

## 指标

对于分类问题，精度是评估模型的指标。精度表示预测的正确率。

对于线性回归问题，平均绝对误差$\frac{1}{m}\sum_{i=1}^m\|h(x)-y\|$是衡量模型的指标。

## 验证集

一般数据分为训练集和测试集，我们还会从训练集中划分出一小部分，称为验证集。验证集的用处是用于调整训练神经网络的参数（比如层数，神经元数，正则化强度等），这类参数称为超参数。

如果数据比较多，可以从训练集中保留一部分数据作为验证集。如果数据不多的话，可以使用K则交叉验证，以及重复的K则交叉验证。这里提到的所有技术，都要求预先打乱数据。

## 过拟合和欠拟合

欠拟合是指模型在训练集、验证集上同时表现不佳，而过拟合是指模型在训练集上表现良好，但是验证集上表现不佳。

随着迭代次数的增加，模型会逐渐由欠拟合转为过拟合。由如下防止过拟合的手段：

- 使用更大的训练集(最优的方法)
- 限制迭代次数
- 使用层次和神经元更少的网络
- 使用L1或L2权重正则化 
- dropout，在一次迭代（一次前向后向传播）使网络中的部分神经元失效（不产生值，也不改变参数）。dropout丢弃操作仅发生在训练阶段，在测试阶段的时候，对应的每一层的输出都乘上一些特殊的修正因子（因为失效神经元了，每一层的输出的总和都变大，因此需要乘上一个较小的数进行缩小）。这样训练出的网络不会过分依赖某个局部特征，而更加倾向于整体。

# 流程

进行机器学习的流程如下：

1. 定义问题。输入是什么，输出是什么，选择的模型。
2. 设置评估的指标。
3. 设置超参数的评估技术，预留验证集，或者使用K则交叉验证。
4. 准备数据。
5. 格式化数据，保证它们的均值为$0$，标准差为$1$。
6. 开发比纯随机的程序更好的模型。

一般的模型选择规则如下：

问题类型|最后一层激活|损失函数
-|-|-
二分类问题|sigmoid|binary_crossentropy
多分类、单标签问题|softmax|categorical_crossentropy
多分类、多标签问题|sigmoid|binary_crossentropy
回归到任意值|无|mse
回归到0-1之间的值|sigmoid|mse或binary_crossentropy

# 卷积神经网络

卷积神经网络不同于一般的神经网络，其会将输入切分为若干个大小相同的块，并对每个块单独生成输出。因此在预测的时候卷积神经网络能够结合附近的信息给出更加精确的预测，并且通过堆叠卷积神经网络，能让神经网络最终预测整个输入。

卷积神经网络非常适合处理图像这种带有局部性的数据。

卷积层一般和池化层合作，卷积层负责从块中提取信息，而池化层负责缩小数据规模。

# 循环神经网络

循环神经网络具备一般神经网络不具备的记忆能力，其实现是通过产生若干层神经网络，并将输入切分成若干份提供给不同的神经网络，并且每一层神经网络同时还接受下一层神经网络的输出，因此每一层神经网络的参数都有两份。

循环神经网络适用于处理时间序列或文本序列这种带有先后关系的数据。

# 参考资料

- 《Python深度学习》