---
categories: technology
layout: post
---

- Table
{:toc}

本文分析的redis版本为6.0。

# 安装配置

## windows安装

从[github](https://github.com/microsoftarchive/redis)上下载windows版本。

之后解压就好了。

# 数据类型

## 字符串

Redis用动态字符串(SDS，Simple dynamic string)的类型来保存字符串。

定义在`sds.h`中的`sdshdr`中。源码在`sds.h`和`sds.c`中。

比较关键的是，当要求分配后的空间为$n$，那么实际分配的空间为$\min(2n,n+2^{20})$，多出来的部分称为预留空间。源码：

```c
sds sdsMakeRoomFor(sds s, size_t addlen) {
...
    newlen = (len+addlen);
    //小于1M就预留相同空间
    if (newlen < SDS_MAX_PREALLOC)
        newlen *= 2;
    else
    //>=1M，则只预留1M
        newlen += SDS_MAX_PREALLOC;
...
}
```

## 链表

定义在`adlist.h`和`adlist.c`文件中。

## 字典

字典的实现为哈希表，通过链表解决冲突。其实现在`dict.h`和`dict.c`文件中。

类似于java中的实现，字典的bucket数目始终为2的幂，这样就可以用且运算来确定哈希值对应的槽了。

```c
//从4开始增加，直到找到2^c，满足2^{c-1}<size 且 2^c>=size
/* Our hash table capability is a power of two */
static unsigned long _dictNextPower(unsigned long size)
{
    unsigned long i = DICT_HT_INITIAL_SIZE;

    if (size >= LONG_MAX) return LONG_MAX + 1LU;
    while(1) {
        if (i >= size)
            return i;
        i *= 2;
    }
}
```

记$\alpha$为哈希表中元素数目与槽数的比例。扩张比较复杂，一般情况下只需要$\alpha\geq 1$就会发生，但是如果正在执行BGSAVE或BGREWRITEAOF等持久化命令，则只有在$\alpha\geq 6$的时候才会发生。这是因为redis在做持久化时会开启一个子进程，通过共享内存来做落盘操作，操作系统对于进程间共享的内存会采用写时复制的技术，因此这期间redis会通过提高扩张的阈值来避免扩张带来的写入操作，从而节约内存。同理当$\alpha<0.1$就会减少bucket，释放内存，但是如果在执行BGSAVE或BGREWRITEAOF等持久化命令，就不会执行缩小操作。

```c
//是否支持缩放，如果元素数/bucket数>dict_force_resize_ratio，则会强制发生哈希，即使不支持缩放
/* Using dictEnableResize() / dictDisableResize() we make possible to
 * enable/disable resizing of the hash table as needed. This is very important
 * for Redis, as we use copy-on-write and don't want to move too much memory
 * around when there is a child performing saving operations.
 *
 * Note that even when dict_can_resize is set to 0, not all resizes are
 * prevented: a hash table is still allowed to grow if the ratio between
 * the number of elements and the buckets > dict_force_resize_ratio. */
static int dict_can_resize = 1;
static unsigned int dict_force_resize_ratio = 5;
...
//缩小哈希表
/* Resize the table to the minimal size that contains all the elements,
 * but with the invariant of a USED/BUCKETS ratio near to <= 1 */
int dictResize(dict *d)
{
    unsigned long minimal;

    //只有在允许缩放以及没有处于哈希中才能进行缩小操作
    if (!dict_can_resize || dictIsRehashing(d)) return DICT_ERR;
    minimal = d->ht[0].used;
    if (minimal < DICT_HT_INITIAL_SIZE)
        minimal = DICT_HT_INITIAL_SIZE;
    return dictExpand(d, minimal);
}
...
//执行一步rehash操作
/* This function performs just a step of rehashing, and only if there are
 * no safe iterators bound to our hash table. When we have iterators in the
 * middle of a rehashing we can't mess with the two hash tables otherwise
 * some element can be missed or duplicated.
 *
 * This function is called by common lookup or update operations in the
 * dictionary so that the hash table automatically migrates from H1 to H2
 * while it is actively used. */
static void _dictRehashStep(dict *d) {
    //这里需要保证没有安全迭代器存在
    if (d->iterators == 0) dictRehash(d,1);
}
...
//按需扩张
/* Expand the hash table if needed */
static int _dictExpandIfNeeded(dict *d)
{
    /* Incremental rehashing already in progress. Return. */
    if (dictIsRehashing(d)) return DICT_OK;

    //这里做初始化操作
    /* If the hash table is empty expand it to the initial size. */
    if (d->ht[0].size == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE);

    /* If we reached the 1:1 ratio, and we are allowed to resize the hash
     * table (global setting) or we should avoid it but the ratio between
     * elements/buckets is over the "safe" threshold, we resize doubling
     * the number of buckets. */
    if (d->ht[0].used >= d->ht[0].size &&
        (dict_can_resize ||
        //由于是整数除法，实际上这里要求达到6倍才行。。
         d->ht[0].used/d->ht[0].size > dict_force_resize_ratio))
    {
        //翻一倍大小
        return dictExpand(d, d->ht[0].used*2);
    }
    return DICT_OK;
}
```

## 跳表

Redis中用跳表来实现有序集合，以及在集群节点中用作内部数据结构。其源码出现在`server.h`和`t_zset.c`文件中。

Redis的跳表的特点是需要同时维护一个哈希表和跳表，其中哈希表存储对象到某个分数的映射，同时负责排重工作，而跳表则支持插入删除排名等操作，其元素按照score作为第一关键字，元素作为第二关键字进行排序。

```c
typedef struct zset {
    //需要同时使用哈希表和skiplist，前者用于查询判重等，后者用于排名，排序等
    dict *dict;
    zskiplist *zsl;
} zset;
```

跳表的元素只能是字符串，为了节省内存，因此哈希表和跳表用的是相同的引用，而释放元素由跳表负责（因此每次删除需要先从哈希表中删除，再删除跳表，否则会存在坏键）。

同时使用哈希表和跳表的主要原因是为了提高性能，哈希表能快速找到某个元素的分数。

跳表的最大高度为32，每个元素的高度由投硬币决定。

```c
//上升概率为0.25
#define ZSKIPLIST_P 0.25      /* Skiplist P = 1/4 */

/* Returns a random level for the new skiplist node we are going to create.
 * The return value of this function is between 1 and ZSKIPLIST_MAXLEVEL
 * (both inclusive), with a powerlaw-alike distribution where higher
 * levels are less likely to be returned. */
int zslRandomLevel(void) {
    //投硬币
    int level = 1;
    while ((random()&0xFFFF) < (ZSKIPLIST_P * 0xFFFF))
        level += 1;
    return (level<ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;
}

```

## 整数集合

如果一个集合只包含整数，并且数量不多，Redis就会用整数集合来存储。其源码出现在`intset.h`和`intset.c`文件中。

```c
typedef struct intset {
    //存储的是几位长度的整数
    uint32_t encoding;
    uint32_t length;
    int8_t contents[];
} intset;
```

整数集合中的元素以从小到大的方式排序存放在`contents`这个柔性数组中，不允许重复元素的出现。

如果放入需要更多字节才能表示的整数，需要对整数集合进行升级，要求重新分配空间。升级后的整数集合并不会因为删除元素而降级。引入升级机制的好处是适用最小的类型来存储整数，这样可以节约内存。

```c
/* Upgrades the intset to a larger encoding and inserts the given integer. */
static intset *intsetUpgradeAndAdd(intset *is, int64_t value) {
    uint8_t curenc = intrev32ifbe(is->encoding);
    uint8_t newenc = _intsetValueEncoding(value);
    int length = intrev32ifbe(is->length);
    //如果是因为过小，就加到前面，否则后面
    int prepend = value < 0 ? 1 : 0;

    /* First set new encoding and resize */
    is->encoding = intrev32ifbe(newenc);
    is = intsetResize(is,intrev32ifbe(is->length)+1);

    /* Upgrade back-to-front so we don't overwrite values.
     * Note that the "prepend" variable is used to make sure we have an empty
     * space at either the beginning or the end of the intset. */
    //从后往前处理，避免覆盖问题
    while(length--)
        _intsetSet(is,length+prepend,_intsetGetEncoded(is,length,curenc));

    /* Set the value at the beginning or the end. */
    //插前面还是后面，这是一个问题
    if (prepend)
        _intsetSet(is,0,value);
    else
        _intsetSet(is,intrev32ifbe(is->length),value);
    is->length = intrev32ifbe(intrev32ifbe(is->length)+1);
    return is;
}
```

当然是用数组来是不可能有有效支持插入删除操作的，这里它们的时间复杂度是$O(n)$。

```c
/* Insert an integer in the intset */
intset *intsetAdd(intset *is, int64_t value, uint8_t *success) {
    uint8_t valenc = _intsetValueEncoding(value);
    uint32_t pos;

    //先默认成功
    if (success) *success = 1;

    /* Upgrade encoding if necessary. If we need to upgrade, we know that
     * this value should be either appended (if > 0) or prepended (if < 0),
     * because it lies outside the range of existing values. */
    if (valenc > intrev32ifbe(is->encoding)) {
        //升级的同时解决插入问题
        /* This always succeeds, so we don't need to curry *success. */
        return intsetUpgradeAndAdd(is,value);
    } else {
        /* Abort if the value is already present in the set.
         * This call will populate "pos" with the right position to insert
         * the value when it cannot be found. */
        //如果存在就返回失败
        if (intsetSearch(is,value,&pos)) {
            if (success) *success = 0;
            return is;
        }

        //???，果真暴力
        is = intsetResize(is,intrev32ifbe(is->length)+1);
        if (pos < intrev32ifbe(is->length)) intsetMoveTail(is,pos,pos+1);
    }

    _intsetSet(is,pos,value);
    is->length = intrev32ifbe(intrev32ifbe(is->length)+1);
    return is;
}
/* Delete integer from intset */
intset *intsetRemove(intset *is, int64_t value, int *success) {
    uint8_t valenc = _intsetValueEncoding(value);
    uint32_t pos;
    if (success) *success = 0;

    if (valenc <= intrev32ifbe(is->encoding) && intsetSearch(is,value,&pos)) {
        uint32_t len = intrev32ifbe(is->length);

        /* We know we can delete */
        if (success) *success = 1;

        //删除也是暴力
        /* Overwrite value with tail and update length */
        if (pos < (len-1)) intsetMoveTail(is,pos+1,pos);
        //每次都需要重新分配内存？？
        is = intsetResize(is,len-1);
        is->length = intrev32ifbe(len-1);
    }
    return is;
}
```

由于是有序存储，因此查找可以通过二分：

```c
/* Search for the position of "value". Return 1 when the value was found and
 * sets "pos" to the position of the value within the intset. Return 0 when
 * the value is not present in the intset and sets "pos" to the position
 * where "value" can be inserted. */
static uint8_t intsetSearch(intset *is, int64_t value, uint32_t *pos) {
    int min = 0, max = intrev32ifbe(is->length)-1, mid = -1;
    int64_t cur = -1;

    //剪枝
    /* The value can never be found when the set is empty */
    if (intrev32ifbe(is->length) == 0) {
        if (pos) *pos = 0;
        return 0;
    } else {
        /* Check for the case where we know we cannot find the value,
         * but do know the insert position. */
        if (value > _intsetGet(is,max)) {
            if (pos) *pos = intrev32ifbe(is->length);
            return 0;
        } else if (value < _intsetGet(is,0)) {
            if (pos) *pos = 0;
            return 0;
        }
    }

    //感人二分
    while(max >= min) {
        mid = ((unsigned int)min + (unsigned int)max) >> 1;
        cur = _intsetGet(is,mid);
        if (value > cur) {
            min = mid+1;
        } else if (value < cur) {
            max = mid-1;
        } else {
            break;
        }
    }


    if (value == cur) {
        if (pos) *pos = mid;
        return 1;
    } else {
        if (pos) *pos = min;
        return 0;
    }
}
```

感觉没什么软用啊。。

## 压缩列表

压缩列表（ziplist）是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表键，且每个列表项要么是小整数，要么是长度较短的字符串，那么Redis就会使用压缩列表来做列表键的底层实现。

压缩列表是以双端链表的方式实现的，因此可以从头部和尾部删除或插入。但是由于每次修改操作都需要将整个压缩链表重新分配，因此时间复杂度总是$O(n)$的。

```c
//每次都需要重新分配内存
/* Resize the ziplist. */
unsigned char *ziplistResize(unsigned char *zl, unsigned int len) {
    zl = zrealloc(zl,len);
    ZIPLIST_BYTES(zl) = intrev32ifbe(len);
    zl[len-1] = ZIP_END;
    return zl;
}
```

ziplist的优点是每个结点都可以有自己独立的压缩模式，因此即使列表中有超大整数，也不会影响较小的数占用非常少的空间。且虽然是双端队列，但是由于是存储了前驱和自身的大小来实现查找前驱后继的功能，因此占用的空间非常小。且由于占用的是连续的空间，因此对缓存友好，且不容易出现内存碎片。

ziplist的中保存的元素的数目是用16位整数存储的，因此当达到最大时，就不会再增长了，之后获取大小必须通过遍历列表实现。

```c
/* Return length of ziplist. */
unsigned int ziplistLen(unsigned char *zl) {
    unsigned int len = 0;
    if (intrev16ifbe(ZIPLIST_LENGTH(zl)) < UINT16_MAX) {
        len = intrev16ifbe(ZIPLIST_LENGTH(zl));
    } else {
        //长度达到$2^{16}$，需要执行扫描才能获得真正的长度
        unsigned char *p = zl+ZIPLIST_HEADER_SIZE;
        while (*p != ZIP_END) {
            p += zipRawEntryLength(p);
            len++;
        }

        /* Re-store length if small enough */
        if (len < UINT16_MAX) ZIPLIST_LENGTH(zl) = intrev16ifbe(len);
    }
    return len;
}

/* Increment the number of items field in the ziplist header. Note that this
 * macro should never overflow the unsigned 16 bit integer, since entries are
 * always pushed one at a time. When UINT16_MAX is reached we want the count
 * to stay there to signal that a full scan is needed to get the number of
 * items inside the ziplist. */
#define ZIPLIST_INCR_LENGTH(zl,incr) { \
    if (ZIPLIST_LENGTH(zl) < UINT16_MAX) \
        ZIPLIST_LENGTH(zl) = intrev16ifbe(intrev16ifbe(ZIPLIST_LENGTH(zl))+incr); \
}
```

## quicklist

学习了ziplist后，可以发现ziplist的优点是占用空间小，但是缺点是所有操作的时间复杂度都是$O(n)$，因此ziplist一旦达到百万级别，队列操作所花的时间就非常多了。而实现队列较好的选择就是linklist，插入弹出时间复杂度均为$O(1)$，但是对应的内存使用量不高。

quicklist结合了linklist和ziplist的优点，我们可以将其理解为对列表的分块，每一块都是一个ziplist，不同的块通过linklist的前驱后继指针的方式串联起来。这样我们就可以较快的实现队列操作，且由于底层的块是由ziplist存储的，因此内存使用量也少。而且由于作为队列时，除了两端的块，中间的块不常被访问，因此可以用压缩算法进行压缩，释放更加多的空间。

可以通过`list-max-ziplist-size`来配置块大小，为正数的时候用于配置每一块最多能包含的元素数目（此时每个ziplist中的元素大小都不能超过8K），负数$-x$表示每一块内存使用量少于$2^{x+1}$KB，$1\leq x\leq 5$。

```c
REDIS_STATIC int
_quicklistNodeSizeMeetsOptimizationRequirement(const size_t sz,
                                               const int fill) {
    //fill>=0表示限制项目数
    if (fill >= 0)
        return 0;

    //<0表示限定内存占用
    size_t offset = (-fill) - 1;
    if (offset < (sizeof(optimization_level) / sizeof(*optimization_level))) {
        if (sz <= optimization_level[offset]) {
            //没有达到上限
            return 1;
        } else {
            return 0;
        }
    } else {
        //使用了未定义的范围
        return 0;
    }
}

REDIS_STATIC int _quicklistNodeAllowInsert(const quicklistNode *node,
                                           const int fill, const size_t sz) {
    if (unlikely(!node))
        return 0;

    int ziplist_overhead;
    /* size of previous offset */
    if (sz < 254)
        ziplist_overhead = 1;
    else
        ziplist_overhead = 5;

    /* size of forward offset */
    if (sz < 64)
        ziplist_overhead += 1;
    else if (likely(sz < 16384))
        ziplist_overhead += 2;
    else
        ziplist_overhead += 5;

    /* new_sz overestimates if 'sz' encodes to an integer type */
    unsigned int new_sz = node->sz + sz + ziplist_overhead;
    //判断新的块大小是否满足占用空间约束
    if (likely(_quicklistNodeSizeMeetsOptimizationRequirement(new_sz, fill)))
        return 1;
    //如果不满足占用空间要求，那么每个元素的最大不能超过8096
    else if (!sizeMeetsSafetyLimit(new_sz))
        return 0;
    //如果fill是非负，那么只要求count不超过fill
    else if ((int)node->count < fill)
        return 1;
    else
        return 0;
}
```

对于压缩，可以通过`list-compress-depth`来配置，若值为$x$，则表示除了最前边的$x$个块和最后边的$x$个块外，其余块全部进行压缩。

```c
//尝试进行压缩
/* Force 'quicklist' to meet compression guidelines set by compress depth.
 * The only way to guarantee interior nodes get compressed is to iterate
 * to our "interior" compress depth then compress the next node we find.
 * If compress depth is larger than the entire list, we return immediately. */
REDIS_STATIC void __quicklistCompress(const quicklist *quicklist,
                                      quicklistNode *node) {
    /* If length is less than our compress depth (from both sides),
     * we can't compress anything. */
    //quicklist->compress为0时表示不压缩
    if (!quicklistAllowsCompression(quicklist) ||
    //剪枝
        quicklist->len < (unsigned int)(quicklist->compress * 2))
        return;

    /* Iterate until we reach compress depth for both sides of the list.a
     * Note: because we do length checks at the *top* of this function,
     *       we can skip explicit null checks below. Everything exists. */
    quicklistNode *forward = quicklist->head;
    quicklistNode *reverse = quicklist->tail;
    int depth = 0;
    //指数变量，表示node是否在两端（压缩部分外）
    int in_depth = 0;

    while (depth++ < quicklist->compress) {
        quicklistDecompressNode(forward);
        quicklistDecompressNode(reverse);

        if (forward == node || reverse == node)
            in_depth = 1;

        if (forward == reverse)
            return;

        forward = forward->next;
        reverse = reverse->prev;
    }

    //需要压缩node
    if (!in_depth)
        quicklistCompressNode(node);

    if (depth > 2) {
        //顺带压缩了？
        /* At this point, forward and reverse are one node beyond depth */
        quicklistCompressNode(forward);
        quicklistCompressNode(reverse);
    }
}
```

# 对象

Redis没有用上面提到的数据结构实现键值对数据库，而是基于这些数据结构创建了一个对象系统。这个系统中包含字符串对象、列表对象、哈希对象、集合对象、有序集合对象共五种类型的对象。

Redis的对象使用引用计数的方式实现了垃圾回收，当某个对象不再被使用的时候就会被自动释放。

Redis的对象还带有访问时间，这样服务器就可以将一些过久没有使用的对象进行淘汰。

Redis使用对象来表示键值对，当我们创建一个新的对象的时候，我们会创建一个SDS用来存储对象名，同时创建一个值对象。

Redis的每个对象都由redisObject所表示，下面是`server.h`文件中的定义：

```c
#define LRU_BITS 24

typedef struct redisObject {
    //值类型
    unsigned type:4;
    //使用的什么数据结构
    unsigned encoding:4;
    //LRU时间，低8位存储频率，高16位存储访问时间
    unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or
                            * LFU data (least significant 8 bits frequency
                            * and most significant 16 bits access time). */
    //引用计数
    int refcount;
    //指向实际的值
    void *ptr;
} robj;
```

其中type字段表示的是什么类型的对象，可能值可以是字符串类型、列表类型、哈希类型、集合类型、有序集合。而encoding则存储了值对应的数据结构，比如即使是列表类型，其也可能是linklist或quicklist。

## 字符串类型

encoding中字符串类型可以分为int,raw,embstr。如果字符串中实际上保存的是一个不超过64位的整数，则会以int类型存储，否则会以raw类型（底层用的sds）存储。但是特殊的如果字符串长度不超过32，那么会用embstr类型存储这个字符串emstr的特点就是将redisObject和sds作为一块连续的区间分配管理，于是这样就减少了内存分配和回收的次数，同时也更好的利用了缓存。在redis中使用字符串类型存储浮点数，而浮点数的字符串形式长度一般不超过32，因此一般会用embstr表示。

## 列表类型

encoding中列表类型可以分为linklist,ziplist,quicklist，其中前两种在Redis3.2版本之前使用，后来quicklist替代了前两者。下面是lpush和rpush的核心实现

```c

/* The function pushes an element to the specified list object 'subject',
 * at head or tail position as specified by 'where'.
 *
 * There is no need for the caller to increment the refcount of 'value' as
 * the function takes care of it if needed. */
void listTypePush(robj *subject, robj *value, int where) {
    if (subject->encoding == OBJ_ENCODING_QUICKLIST) {
        int pos = (where == LIST_HEAD) ? QUICKLIST_HEAD : QUICKLIST_TAIL;
        //对value进行解码，并增加引用计数
        value = getDecodedObject(value);
        size_t len = sdslen(value->ptr);
        quicklistPush(subject->ptr, value->ptr, len, pos);
        //减少引用计数，释放解码后的对象（quicklist会拷贝值）
        decrRefCount(value);
    } else {
        serverPanic("Unknown list encoding");
    }
}

//where表示头部还是尾部加
void pushGenericCommand(client *c, int where) {
    int j, pushed = 0;

    //找到对象
    robj *lobj = lookupKeyWrite(c->db,c->argv[1]);

    if (lobj && lobj->type != OBJ_LIST) {
        addReply(c,shared.wrongtypeerr);
        return;
    }

    //遍历待加对象
    for (j = 2; j < c->argc; j++) {
        if (!lobj) {
            //如果是新对象，就创建一个quicklist对象
            lobj = createQuicklistObject();
            //设置fill和depth参数
            quicklistSetOptions(lobj->ptr, server.list_max_ziplist_size,
                                server.list_compress_depth);
            //将新对象注册到数据库中
            dbAdd(c->db,c->argv[1],lobj);
        }

        listTypePush(lobj,c->argv[j],where);
        pushed++;
    }

    //回复长度
    addReplyLongLong(c, (lobj ? listTypeLength(lobj) : 0));
    if (pushed) {
        //推送事件
        char *event = (where == LIST_HEAD) ? "lpush" : "rpush";

        signalModifiedKey(c,c->db,c->argv[1]);
        notifyKeyspaceEvent(NOTIFY_LIST,event,c->argv[1],c->db->id);
    }
    //修改版本
    server.dirty += pushed;
}
```

## 哈希类型

redis中哈希类型的编码可能为ziplist和hashtable。ziplist中将键值对作为表中的两个项紧密排放，之后每次查询都暴力查表。新加入的元素会插入到ziplist的尾部。而hashtable编码的哈希类型底层使用dict存储键值对。

```c
//哈希类型默认使用ziplist编码
robj *createHashObject(void) {
    unsigned char *zl = ziplistNew();
    robj *o = createObject(OBJ_HASH, zl);
    o->encoding = OBJ_ENCODING_ZIPLIST;
    return o;
}
```

ziplist会因为存储的元素占用过大空间和ziplist长度过大原因升级为哈希表。

```c
//判断是否需要将ziplist转哈希表
/* Check the length of a number of objects to see if we need to convert a
 * ziplist to a real hash. Note that we only check string encoded objects
 * as their string length can be queried in constant time. */
void hashTypeTryConversion(robj *o, robj **argv, int start, int end) {
    int i;

    if (o->encoding != OBJ_ENCODING_ZIPLIST) return;

    for (i = start; i <= end; i++) {
        if (sdsEncodedObject(argv[i]) &&
            sdslen(argv[i]->ptr) > server.hash_max_ziplist_value)
        {
          //如果键值中有一个超出server.hash_max_ziplist_value就转哈希表
          hashTypeConvert(o, OBJ_ENCODING_HT);
          break;
        }
    }
}

//哈希表set操作
int hashTypeSet(robj *o, sds field, sds value, int flags) {
    int update = 0;

    if (o->encoding == OBJ_ENCODING_ZIPLIST) {
        unsigned char *zl, *fptr, *vptr;

        ...

        /* Check if the ziplist needs to be converted to a hash table */
        if (hashTypeLength(o) > server.hash_max_ziplist_entries)
          //如果ziplist项数超过server.hash_max_ziplist_entries也要转哈希表
          hashTypeConvert(o, OBJ_ENCODING_HT);
    } 
    ...
}
```

## 集合对象

集合对象的编码可以是intset或hashtable。

```c
/* Factory method to return a set that *can* hold "value". When the object has
 * an integer-encodable value, an intset will be returned. Otherwise a regular
 * hash table. */
//创建一个新的集合
robj *setTypeCreate(sds value) {
    if (isSdsRepresentableAsLongLong(value,NULL) == C_OK)
        //如果插入的新值可以编码为整数，就创建intset
        return createIntsetObject();
    //否则还是建一个集合吧
    return createSetObject();
}

//集合类型用哈希表来实现
robj *createSetObject(void) {
    dict *d = dictCreate(&setDictType,NULL);
    robj *o = createObject(OBJ_SET,d);
    o->encoding = OBJ_ENCODING_HT;
    return o;
}
```

intset在元素类型不能以64位整数表示，以及长度过大的时候都会自动转哈希表。

```c
    //新增操作
    if (subject->encoding == OBJ_ENCODING_INTSET) {
        if (isSdsRepresentableAsLongLong(value,&llval) == C_OK) {
            uint8_t success = 0;
            subject->ptr = intsetAdd(subject->ptr,llval,&success);
            if (success) {
                /* Convert to regular set when the intset contains
                 * too many entries. */
                if (intsetLen(subject->ptr) > server.set_max_intset_entries)
                  //如果整数集合大小超过server.set_max_intset_entries，转哈希表
                  setTypeConvert(subject, OBJ_ENCODING_HT);
                return 1;
            }
        } else {
            //如果新元素不能表示为64位整数，转哈希表
            /* Failed to get integer from object, convert to regular set. */
            setTypeConvert(subject,OBJ_ENCODING_HT);

            /* The set *was* an intset and this value is not integer
             * encodable, so dictAdd should always work. */
            serverAssert(dictAdd(subject->ptr,sdsdup(value),NULL) == DICT_OK);
            return 1;
        }
    }
```

当使用哈希表的时候，键为集合中的元素，值为NULL。

```c
        dict *ht = subject->ptr;
        dictEntry *de = dictAddRaw(ht,value,NULL);
        if (de) {
            dictSetKey(ht,de,sdsdup(value));
            dictSetVal(ht,de,NULL);
            return 1;
        }
```

## 有序集合类型

有序列表的编码可以是ziplist或skiplist。

```c
/* Lookup the key and create the sorted set if does not exist. */
    zobj = lookupKeyWrite(c->db,key);
    if (zobj == NULL) {
        if (xx) goto reply_to_client; /* No key + XX option: nothing to do. */
        if (server.zset_max_ziplist_entries == 0 ||
            server.zset_max_ziplist_value < sdslen(c->argv[scoreidx+1]->ptr))
        {
          //如果第一个新元素就要求升级，直接上zset
          zobj = createZsetObject();
        } else {
            //否则创建一个ziplist凑合用
            zobj = createZsetZiplistObject();
        }
        dbAdd(c->db,key,zobj);
    } 
```

ziplist中的元素按从小到大排序。

```c
/* Update the sorted set according to its encoding. */
    if (zobj->encoding == OBJ_ENCODING_ZIPLIST) {
        unsigned char *eptr;

        //找到插入位置
        if ((eptr = zzlFind(zobj->ptr,ele,&curscore)) != NULL) {
            /* NX? Return, same element already exists. */
            if (nx) {
                *flags |= ZADD_NOP;
                return 1;
            }

            /* Prepare the score for the increment if needed. */
            if (incr) {
                score += curscore;
                if (isnan(score)) {
                    *flags |= ZADD_NAN;
                    return 0;
                }
                if (newscore) *newscore = score;
            }

            /* Remove and re-insert when score changed. */
            if (score != curscore) {
                zobj->ptr = zzlDelete(zobj->ptr,eptr);
                zobj->ptr = zzlInsert(zobj->ptr,ele,score);
                *flags |= ZADD_UPDATED;
            }
            return 1;
        } else if (!xx) {
            /* Optimize: check if the element is too large or the list
             * becomes too long *before* executing zzlInsert. */
            zobj->ptr = zzlInsert(zobj->ptr,ele,score);
            ...
        }
    }
```

可以发现ziplist当过长或元素过大的时候，就会升级为skiplist。

```c
...
//过大或过长？
if (zzlLength(zobj->ptr) > server.zset_max_ziplist_entries ||
                sdslen(ele) > server.zset_max_ziplist_value)
                zsetConvert(zobj,OBJ_ENCODING_SKIPLIST);
            if (newscore) *newscore = score;
            *flags |= ZADD_ADDED;
            return 1;
```

# 内存回收

C语言不自带内存回收，redis通过引用计数的方式自己实现了内存回收机制。

```c
typedef struct redisObject {
...
    //引用计数
    int refcount;
...
} robj;
```

当创建一个新对象的时候，其计数为1。在对象被引用的时候，其计数会加1，当对象不再被引用时，其计数会减1。而当对象计数为0的时候，对象就会被释放。

# 空转时长

Redis中采用lru字段记录对象最后一次被客户端访问的时间。

```c
typedef struct redisObject {
    //LRU时间，低8位存储频率，高16位存储访问时间
    unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or
                            * LFU data (least significant 8 bits frequency
                            * and most significant 16 bits access time). */
} robj;
```

当我们设置了maxmemory选项，且服务器的淘汰算法为voltile-lruh或者allkeys-lru。那么当服务器占用了超过maxmemory的内存，那么就会优先释放那些空转时长较大的对象。

# 数据库

## 数据库对象

服务器对象对应的是`redisServer`对象。

Redis服务器中所有数据库都保存在`redisServer`结构的db数组中。下面是`server.h`中的源码：

```c
struct redisServer { 
...
    redisDb *db;
    int dbnum;                      /* Total number of configured DBs */
...
};
```

默认情况下客户端使用的数据库是0号数据库，客户端可以通过SELECT命令来切换数据库。

redis的数据库对象的源码如下，它的dict字段来保存对象名称到对象的映射，这些映射称为键空间。

```c
/* Redis database representation. There are multiple databases identified
 * by integers from 0 (the default database) up to the max configured
 * database. The database number is the 'id' field in the structure. */
typedef struct redisDb {
    //键空间
    dict *dict;                 /* The keyspace for this DB */
    //所有键的过期时间
    dict *expires;              /* Timeout of keys with a timeout set */
    dict *blocking_keys;        /* Keys with clients waiting for data (BLPOP)*/
    dict *ready_keys;           /* Blocked keys that received a PUSH */
    dict *watched_keys;         /* WATCHED keys for MULTI/EXEC CAS */
    int id;                     /* Database ID */
    long long avg_ttl;          /* Average TTL, just for stats */
    unsigned long expires_cursor; /* Cursor of the active expire cycle. */
    list *defrag_later;         /* List of key names to attempt to defrag one by one, gradually. */
} redisDb;
```

## 客户端对象

客户端对象对应的是`client`结构。

```c
struct client { 
    //使用哪个数据库
    redisDb *db;            /* Pointer to currently SELECTed DB. */
}
```


## 过期

expires成员中保存了所有的键的过期时间，过期时间是以毫秒为精度的UNIX时间戳。

```c
/* Redis database representation. There are multiple databases identified
 * by integers from 0 (the default database) up to the max configured
 * database. The database number is the 'id' field in the structure. */
typedef struct redisDb {
    //键空间
    dict *dict;                 /* The keyspace for this DB */
    //所有键的过期时间
    dict *expires;              /* Timeout of keys with a timeout set */
} redisDb;
```

可以通过expire命令设置过期时间。

```c
/* Set an expire to the specified key. If the expire is set in the context
 * of an user calling a command 'c' is the client, otherwise 'c' is set
 * to NULL. The 'when' parameter is the absolute unix time in milliseconds
 * after which the key will no longer be considered valid. */
void setExpire(client *c, redisDb *db, robj *key, long long when) {
    dictEntry *kde, *de;

    /* Reuse the sds from the main dict in the expire dict */
    kde = dictFind(db->dict,key->ptr);
    serverAssertWithInfo(NULL,key,kde != NULL);
    //设置过期时间（用同一个key对象）
    de = dictAddOrFind(db->expires,dictGetKey(kde));
    dictSetSignedIntegerVal(de,when);

    int writable_slave = server.masterhost && server.repl_slave_ro == 0;
    if (c && writable_slave && !(c->flags & CLIENT_MASTER))
        rememberSlaveKeyWithExpire(db,key);
}

/* This is the generic command implementation for EXPIRE, PEXPIRE, EXPIREAT
 * and PEXPIREAT. Because the commad second argument may be relative or absolute
 * the "basetime" argument is used to signal what the base time is (either 0
 * for *AT variants of the command, or the current time for relative expires).
 *
 * unit is either UNIT_SECONDS or UNIT_MILLISECONDS, and is only used for
 * the argv[2] parameter. The basetime is always specified in milliseconds. */
void expireGenericCommand(client *c, long long basetime, int unit) {
    robj *key = c->argv[1], *param = c->argv[2];
    long long when; /* unix time in milliseconds when the key will expire. */

    if (getLongLongFromObjectOrReply(c, param, &when, NULL) != C_OK)
        return;

    if (unit == UNIT_SECONDS) when *= 1000;
    when += basetime;

    /* No key, return zero. */
    if (lookupKeyWrite(c->db,key) == NULL) {
        //找不到key
        addReply(c,shared.czero);
        return;
    }

    /* EXPIRE with negative TTL, or EXPIREAT with a timestamp into the past
     * should never be executed as a DEL when load the AOF or in the context
     * of a slave instance.
     *
     * Instead we take the other branch of the IF statement setting an expire
     * (possibly in the past) and wait for an explicit DEL from the master. */
    if (when <= mstime() && !server.loading && !server.masterhost) {
        //在过去过期，就是删除啦
        robj *aux;

        int deleted = server.lazyfree_lazy_expire ? dbAsyncDelete(c->db,key) :
                                                    dbSyncDelete(c->db,key);
        serverAssertWithInfo(c,key,deleted);
        server.dirty++;

        /* Replicate/AOF this as an explicit DEL or UNLINK. */
        aux = server.lazyfree_lazy_expire ? shared.unlink : shared.del;
        rewriteClientCommandVector(c,2,aux,key);
        signalModifiedKey(c,c->db,key);
        notifyKeyspaceEvent(NOTIFY_GENERIC,"del",key,c->db->id);
        addReply(c, shared.cone);
        return;
    } else {
        //设一下过期时间
        setExpire(c,c->db,key,when);
        addReply(c,shared.cone);
        signalModifiedKey(c,c->db,key);
        notifyKeyspaceEvent(NOTIFY_GENERIC,"expire",key,c->db->id);
        server.dirty++;
        return;
    }
}
```

也可以用persist命令去除某个对象的过期时间。

## 删除

键的删除分为定时删除和惰性删除两种。

可以使用`lazyfree_lazy_server_del`开启异步删除。如果对象比较大且是立即被释放（引用计数为1），则会走异步，否则释放走同步。

```c
/* This is a wrapper whose behavior depends on the Redis lazy free
 * configuration. Deletes the key synchronously or asynchronously. */
 //删除对象
int dbDelete(redisDb *db, robj *key) {
    return server.lazyfree_lazy_server_del ? dbAsyncDelete(db,key) :
                                             dbSyncDelete(db,key);
}

//异步删除
int dbAsyncDelete(redisDb *db, robj *key) {
    //从过期时间中删除key
    /* Deleting an entry from the expires dict will not free the sds of
     * the key, because it is shared with the main dictionary. */
    if (dictSize(db->expires) > 0) dictDelete(db->expires,key->ptr);

    /* If the value is composed of a few allocations, to free in a lazy way
     * is actually just slower... So under a certain limit we just free
     * the object synchronously. */
    dictEntry *de = dictUnlink(db->dict,key->ptr);
    if (de) {
        //存在于数据库
        robj *val = dictGetVal(de);
        //大概删除费时
        size_t free_effort = lazyfreeGetFreeEffort(val);

        /* If releasing the object is too much work, do it in the background
         * by adding the object to the lazy free list.
         * Note that if the object is shared, to reclaim it now it is not
         * possible. This rarely happens, however sometimes the implementation
         * of parts of the Redis core may call incrRefCount() to protect
         * objects, and then call dbDelete(). In this case we'll fall
         * through and reach the dictFreeUnlinkedEntry() call, that will be
         * equivalent to just calling decrRefCount(). */
        if (free_effort > LAZYFREE_THRESHOLD && val->refcount == 1) {
          //首先真的会删除，其次删除花的时间达到LAZYFREE_THRESHOLD才执行异步，否则就是浪费时间
          atomicIncr(lazyfree_objects, 1);
          //插入异步任务
          bioCreateBackgroundJob(BIO_LAZY_FREE, val, NULL, NULL);
          //值异步释放，因此这里把值设成NULL防止被后面的同步操作锁释放
          dictSetVal(db->dict, de, NULL);
        }
    }

    /* Release the key-val pair, or just the key if we set the val
     * field to NULL in order to lazy free it later. */
    if (de) {
        dictFreeUnlinkedEntry(db->dict,de);
        if (server.cluster_enabled) slotToKeyDel(key->ptr);
        return 1;
    } else {
        return 0;
    }
}
```

惰性删除是指每次客户端访问某个对象前都需要检查键是否过期。

```c
/* This function is called when we are going to perform some operation
 * in a given key, but such key may be already logically expired even if
 * it still exists in the database. The main way this function is called
 * is via lookupKey*() family of functions.
 *
 * The behavior of the function depends on the replication role of the
 * instance, because slave instances do not expire keys, they wait
 * for DELs from the master for consistency matters. However even
 * slaves will try to have a coherent return value for the function,
 * so that read commands executed in the slave side will be able to
 * behave like if the key is expired even if still present (because the
 * master has yet to propagate the DEL).
 *
 * In masters as a side effect of finding a key which is expired, such
 * key will be evicted from the database. Also this may trigger the
 * propagation of a DEL/UNLINK command in AOF / replication stream.
 *
 * The return value of the function is 0 if the key is still valid,
 * otherwise the function returns 1 if the key is expired. */
int expireIfNeeded(redisDb *db, robj *key) {
    //未过期
    if (!keyIsExpired(db,key)) return 0;

    /* If we are running in the context of a slave, instead of
     * evicting the expired key from the database, we return ASAP:
     * the slave key expiration is controlled by the master that will
     * send us synthesized DEL operations for expired keys.
     *
     * Still we try to return the right information to the caller,
     * that is, 0 if we think the key should be still valid, 1 if
     * we think the key is expired at this time. */
    //slave过期了不执行删除，但是只返回正确信息
    if (server.masterhost != NULL) return 1;

    /* Delete the key */
    //真的删除
    server.stat_expiredkeys++;
    propagateExpire(db,key,server.lazyfree_lazy_expire);
    notifyKeyspaceEvent(NOTIFY_EXPIRED,
        "expired",key,db->id);
    //同步还是异步删除
    int retval = server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) :
                                               dbSyncDelete(db,key);
    if (retval) signalModifiedKey(NULL,db,key);
    return retval;
}

/* Lookup a key for read operations, or return NULL if the key is not found
 * in the specified DB.
 *
 * As a side effect of calling this function:
 * 1. A key gets expired if it reached it's TTL.
 * 2. The key last access time is updated.
 * 3. The global keys hits/misses stats are updated (reported in INFO).
 * 4. If keyspace notifications are enabled, a "keymiss" notification is fired.
 *
 * This API should not be used when we write to the key after obtaining
 * the object linked to the key, but only for read only operations.
 *
 * Flags change the behavior of this command:
 *
 *  LOOKUP_NONE (or zero): no special flags are passed.
 *  LOOKUP_NOTOUCH: don't alter the last access time of the key.
 *
 * Note: this function also returns NULL if the key is logically expired
 * but still existing, in case this is a slave, since this API is called only
 * for read operations. Even if the key expiry is master-driven, we can
 * correctly report a key is expired on slaves even if the master is lagging
 * expiring our key via DELs in the replication link. */
robj *lookupKeyReadWithFlags(redisDb *db, robj *key, int flags) {
    robj *val;

    if (expireIfNeeded(db,key) == 1) {
    ...
    }
    ...
}


/* Lookup a key for write operations, and as a side effect, if needed, expires
 * the key if its TTL is reached.
 *
 * Returns the linked value object if the key exists or NULL if the key
 * does not exist in the specified DB. */
robj *lookupKeyWriteWithFlags(redisDb *db, robj *key, int flags) {
    expireIfNeeded(db,key);
    return lookupKey(db,key,flags);
}

```

定时删除是指服务器周期性地遍历各个数据库，从每个数据库的expires字典中随机检查一部分键的过期时间并删除其中的过期键。

```c
void activeExpireCycle(int type) {
    /* Adjust the running parameters according to the configured expire
     * effort. The default effort is 1, and the maximum configurable effort
     * is 10. */
    unsigned long
    effort = server.active_expire_effort-1, /* Rescale from 0 to 9. */
    config_keys_per_loop = ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP +
                           ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP/4*effort,
    config_cycle_fast_duration = ACTIVE_EXPIRE_CYCLE_FAST_DURATION +
                                 ACTIVE_EXPIRE_CYCLE_FAST_DURATION/4*effort,
    config_cycle_slow_time_perc = ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC +
                                  2*effort,
    config_cycle_acceptable_stale = ACTIVE_EXPIRE_CYCLE_ACCEPTABLE_STALE-
                                    effort;

    /* This function has some global state in order to continue the work
     * incrementally across calls. */
    //上一个扫描过的DB下标
    static unsigned int current_db = 0; /* Last DB tested. */
    static int timelimit_exit = 0;      /* Time limit hit in previous call? */
    static long long last_fast_cycle = 0; /* When last fast cycle ran. */

    int j, iteration = 0;
    int dbs_per_call = CRON_DBS_PER_CALL;
    long long start = ustime(), timelimit, elapsed;

    /* When clients are paused the dataset should be static not just from the
     * POV of clients not being able to write, but also from the POV of
     * expires and evictions of keys not being performed. */
    if (clientsArePaused()) return;

    if (type == ACTIVE_EXPIRE_CYCLE_FAST) {
        /* Don't start a fast cycle if the previous cycle did not exit
         * for time limit, unless the percentage of estimated stale keys is
         * too high. Also never repeat a fast cycle for the same period
         * as the fast cycle total duration itself. */
        if (!timelimit_exit &&
            server.stat_expired_stale_perc < config_cycle_acceptable_stale)
            return;

        if (start < last_fast_cycle + (long long)config_cycle_fast_duration*2)
            return;

        last_fast_cycle = start;
    }

    /* We usually should test CRON_DBS_PER_CALL per iteration, with
     * two exceptions:
     *
     * 1) Don't test more DBs than we have.
     * 2) If last time we hit the time limit, we want to scan all DBs
     * in this iteration, as there is work to do in some DB and we don't want
     * expired keys to use memory for too much time. */
    if (dbs_per_call > server.dbnum || timelimit_exit)
        dbs_per_call = server.dbnum;

    /* We can use at max 'config_cycle_slow_time_perc' percentage of CPU
     * time per iteration. Since this function gets called with a frequency of
     * server.hz times per second, the following is the max amount of
     * microseconds we can spend in this function. */
    timelimit = config_cycle_slow_time_perc*1000000/server.hz/100;
    timelimit_exit = 0;
    if (timelimit <= 0) timelimit = 1;

    if (type == ACTIVE_EXPIRE_CYCLE_FAST)
        timelimit = config_cycle_fast_duration; /* in microseconds. */

    /* Accumulate some global stats as we expire keys, to have some idea
     * about the number of keys that are already logically expired, but still
     * existing inside the database. */
    long total_sampled = 0;
    long total_expired = 0;

    for (j = 0; j < dbs_per_call && timelimit_exit == 0; j++) {
        /* Expired and checked in a single loop. */
        unsigned long expired, sampled;

        //选择下一个db
        redisDb *db = server.db+(current_db % server.dbnum);

        /* Increment the DB now so we are sure if we run out of time
         * in the current DB we'll restart from the next. This allows to
         * distribute the time evenly across DBs. */
        current_db++;

        /* Continue to expire if at the end of the cycle there are still
         * a big percentage of keys to expire, compared to the number of keys
         * we scanned. The percentage, stored in config_cycle_acceptable_stale
         * is not fixed, but depends on the Redis configured "expire effort". */
        do {
            unsigned long num, slots;
            long long now, ttl_sum;
            int ttl_samples;
            iteration++;

            /* If there is nothing to expire try next DB ASAP. */
            //不存在允许过期的键
            if ((num = dictSize(db->expires)) == 0) {
                db->avg_ttl = 0;
                break;
            }
            slots = dictSlots(db->expires);
            now = mstime();

            
            //如果expire表很稀松（数据量/slot < 0.01）就跳出
            /* When there are less than 1% filled slots, sampling the key
             * space is expensive, so stop here waiting for better times...
             * The dictionary will be resized asap. */
            if (num && slots > DICT_HT_INITIAL_SIZE &&
                (num*100/slots < 1)) break;

            /* The main collection cycle. Sample random keys among keys
             * with an expire set, checking for expired ones. */
            expired = 0;
            sampled = 0;
            ttl_sum = 0;
            ttl_samples = 0;

            //采样数上限为min(dictSize(db->expires), config_keys_per_loop)
            if (num > config_keys_per_loop)
                num = config_keys_per_loop;

            /* Here we access the low level representation of the hash table
             * for speed concerns: this makes this code coupled with dict.c,
             * but it hardly changed in ten years.
             *
             * Note that certain places of the hash table may be empty,
             * so we want also a stop condition about the number of
             * buckets that we scanned. However scanning for free buckets
             * is very fast: we are in the cache line scanning a sequential
             * array of NULL pointers, so we can scan a lot more buckets
             * than keys in the same time. */
            long max_buckets = num*20;
            long checked_buckets = 0;

            while (sampled < num && checked_buckets < max_buckets) {
                //还没采够样
                for (int table = 0; table < 2; table++) {
                    if (table == 1 && !dictIsRehashing(db->expires)) break;

                    unsigned long idx = db->expires_cursor;
                    idx &= db->expires->ht[table].sizemask;
                    dictEntry *de = db->expires->ht[table].table[idx];
                    long long ttl;

                    /* Scan the current bucket of the current table. */
                    checked_buckets++;
                    //遍历链表
                    while(de) {
                        /* Get the next entry now since this entry may get
                         * deleted. */
                        dictEntry *e = de;
                        de = de->next;

                        ttl = dictGetSignedIntegerVal(e)-now;
                        //找到可能过期的键
                        if (activeExpireCycleTryExpire(db,e,now)) expired++;
                        if (ttl > 0) {
                            /* We want the average TTL of keys yet
                             * not expired. */
                            //统计所有过期时间的总和来计算平均数
                            ttl_sum += ttl;
                            ttl_samples++;
                        }
                        //又采了个样
                        sampled++;
                    }
                }
                db->expires_cursor++;
            }
            total_expired += expired;
            total_sampled += sampled;

            /* Update the average TTL stats for this database. */
            //更新平均TTL数据
            if (ttl_samples) {
                long long avg_ttl = ttl_sum/ttl_samples;

                /* Do a simple running average with a few samples.
                 * We just use the current estimate with a weight of 2%
                 * and the previous estimate with a weight of 98%. */
                if (db->avg_ttl == 0) db->avg_ttl = avg_ttl;
                db->avg_ttl = (db->avg_ttl/50)*49 + (avg_ttl/50);
            }

            /* We can't block forever here even if there are many keys to
             * expire. So after a given amount of milliseconds return to the
             * caller waiting for the other active expire cycle. */
            //如果超时了，就赶紧退出吧
            if ((iteration & 0xf) == 0) { /* check once every 16 iterations. */
                elapsed = ustime()-start;
                if (elapsed > timelimit) {
                    timelimit_exit = 1;
                    server.stat_expired_time_cap_reached_count++;
                    break;
                }
            }
            /* We don't repeat the cycle for the current database if there are
             * an acceptable amount of stale keys (logically expired but yet
             * not reclaimed). */
        } while (sampled == 0 ||
                 (expired*100/sampled) > config_cycle_acceptable_stale);
    }

    
    elapsed = ustime()-start;
    server.stat_expire_cycle_time_used += elapsed;
    latencyAddSampleIfNeeded("expire-cycle",elapsed/1000);

    /* Update our estimate of keys existing but yet to be expired.
     * Running average with this sample accounting for 5%. */
    double current_perc;
    if (total_sampled) {
        current_perc = (double)total_expired/total_sampled;
    } else
        current_perc = 0;
    server.stat_expired_stale_perc = (current_perc*0.05)+
                                     (server.stat_expired_stale_perc*0.95);
}
```

## 订阅

redis支持订阅功能，我们可以订阅

# 持久化

由于redis是内存数据库，因此重启后所有对象都会丢失。redis提供了一些持久化的方案。

## RDB

执行SAVE和BGSAVE命令的时候，会将Redis中的对象导出为一个`dump.rdb`文件。这里我看到一些书上说是服务器只会导出未过期的关键字，redis源码的注释上也这么写，但是代码里好像没有做过滤，这里存疑。

```c
/* Save a key-value pair, with expire time, type, key, value.
 * On error -1 is returned.
 * On success if the key was actually saved 1 is returned, otherwise 0
 * is returned (the key was already expired). */
//但是看代码好像过期对象也会写入到rdb中。
int rdbSaveKeyValuePair(rio *rdb, robj *key, robj *val, long long expiretime) {
    int savelru = server.maxmemory_policy & MAXMEMORY_FLAG_LRU;
    int savelfu = server.maxmemory_policy & MAXMEMORY_FLAG_LFU;

    /* Save the expire time */
    if (expiretime != -1) {
        //保存过期时间
        if (rdbSaveType(rdb,RDB_OPCODE_EXPIRETIME_MS) == -1) return -1;
        if (rdbSaveMillisecondTime(rdb,expiretime) == -1) return -1;
    }

    /* Save the LRU info. */
    if (savelru) {
        uint64_t idletime = estimateObjectIdleTime(val);
        idletime /= 1000; /* Using seconds is enough and requires less space.*/
        if (rdbSaveType(rdb,RDB_OPCODE_IDLE) == -1) return -1;
        if (rdbSaveLen(rdb,idletime) == -1) return -1;
    }

    /* Save the LFU info. */
    if (savelfu) {
        uint8_t buf[1];
        buf[0] = LFUDecrAndReturn(val);
        /* We can encode this in exactly two bytes: the opcode and an 8
         * bit counter, since the frequency is logarithmic with a 0-255 range.
         * Note that we do not store the halving time because to reset it
         * a single time when loading does not affect the frequency much. */
        if (rdbSaveType(rdb,RDB_OPCODE_FREQ) == -1) return -1;
        if (rdbWriteRaw(rdb,buf,1) == -1) return -1;
    }

    /* Save type, key, value */
    if (rdbSaveObjectType(rdb,val) == -1) return -1;
    if (rdbSaveStringObject(rdb,key) == -1) return -1;
    if (rdbSaveObject(rdb,val,key) == -1) return -1;

    /* Delay return if required (for testing) */
    if (server.rdb_key_save_delay)
        //挂起一段时间，貌似用于测试
        usleep(server.rdb_key_save_delay);

    return 1;
}

```

之后重启服务器后会自动识别rdb文件并从rdb文件中恢复（没有加载命令）。

SAVE是同步操作，BGSAVE是异步操作。执行SAVE会导致服务器在导出的过程中阻塞，不会接受任何客户端的请求。而BGSAVE实际上会fork当前进程创建一个子进程，之后由子进程通过共享内存的方式导出所有对象。

比较特殊的是，如果运行的服务器是主服务器，则加载RDB文件时不会恢复过期的键，而从服务器则会将所有键全部恢复。

注意，由于AOF文件一般比RDB文件更接近最后的版本，因此如果服务器开启了AOF功能，那么启动时会优先利用AOF文件恢复。只有在AOF功能关闭的情况下，才会选择使用RDB文件恢复。

在执行BGSAVE命令的时候，如果客户端发来SAVE或BGSAVE命令都会被拒绝。

```c
/* BGSAVE [SCHEDULE] */
//异步dump
void bgsaveCommand(client *c) {
    ...
    if (server.rdb_child_pid != -1) {
        //已经有任务在跑了
        addReplyError(c,"Background save already in progress");
    }
    ...
}

void saveCommand(client *c) {
    if (server.rdb_child_pid != -1) {
        addReplyError(c,"Background save already in progress");
        return;
    }
    ...
}
```

BGSAVE也可以定时执行。当距离上一次修改时间够大，且修改量足够大，就会触发BGSAVE任务（当然前提是没有其它子进程存活）。

```c
int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {
    ...
  /* Check if a background saving or AOF rewrite in progress terminated. */
    if (hasActiveChildProcess() || ldbPendingChildren())
    {
        checkChildrenDone();
    } else {
        /* If there is not a background saving/rewrite in progress check if
         * we have to save/rewrite now. */
         //如果没有子进程在跑，就看一下需不需要启动一个子进程
        for (j = 0; j < server.saveparamslen; j++) {
            struct saveparam *sp = server.saveparams+j;

            /* Save if we reached the given amount of changes,
             * the given amount of seconds, and if the latest bgsave was
             * successful or if, in case of an error, at least
             * CONFIG_BGSAVE_RETRY_DELAY seconds already elapsed. */
            if (server.dirty >= sp->changes &&
                server.unixtime-server.lastsave > sp->seconds &&
                (server.unixtime-server.lastbgsave_try >
                 CONFIG_BGSAVE_RETRY_DELAY ||
                 server.lastbgsave_status == C_OK))
            {
                serverLog(LL_NOTICE,"%d changes in %d seconds. Saving...",
                    sp->changes, (int)sp->seconds);
                rdbSaveInfo rsi, *rsiptr;
                rsiptr = rdbPopulateSaveInfo(&rsi);

                //于是乎启动了一个
                rdbSaveBackground(server.rdb_filename,rsiptr);
                break;
            }
        }
    }
    ...
}
```

可以通过`config set save`来设置saveparam。

```c
struct redisServer {
    struct saveparam *saveparams;   /* Save points array for RDB */
};

struct saveparam {
    time_t seconds;
    int changes;
};
```

saveparam在配置文件中的默认值。

```properties
################################ SNAPSHOTTING  ################################
#
# Save the DB on disk:
#
#   save <seconds> <changes>
#
#   Will save the DB if both the given number of seconds and the given
#   number of write operations against the DB occurred.
#
#   In the example below the behaviour will be to save:
#   after 900 sec (15 min) if at least 1 key changed
#   after 300 sec (5 min) if at least 10 keys changed
#   after 60 sec if at least 10000 keys changed
#
#   Note: you can disable saving completely by commenting out all "save" lines.
#
#   It is also possible to remove all the previously configured save
#   points by adding a save directive with a single empty string argument
#   like in the following example:
#
#   save ""

save 900 1
save 300 10
save 60 10000
```

## AOF

RDB文件是通过dump整个服务器来实现持久化，AOF是通过将写命令落日志来实现持久化。

当AOF功能打开后，服务器在执行完一个写命令后，会将写命令追加到服务的`aof_buf`缓冲区中。

```c
struct redisServer {
    sds aof_buf;      /* AOF buffer, written before entering the event loop */
};
```

AOF的刷盘策略有三种，可以在配置文件中配置。默认值是everysec表示每秒同步一次，还有always和no，always表示每次写入请求都会落盘，而no则表示让操作系统自由决定什么时候落盘。no的性能最快，everysec会将落盘操作作为异步任务执行，因此速度其次，最慢的是always。

```properties
# The fsync() call tells the Operating System to actually write data on disk
# instead of waiting for more data in the output buffer. Some OS will really flush
# data on disk, some other OS will just try to do it ASAP.
#
# Redis supports three different modes:
#
# no: don't fsync, just let the OS flush the data when it wants. Faster.
# always: fsync after every write to the append only log. Slow, Safest.
# everysec: fsync only one time every second. Compromise.
#
# The default is "everysec", as that's usually the right compromise between
# speed and data safety. It's up to you to understand if you can relax this to
# "no" that will let the operating system flush the output buffer when
# it wants, for better performances (but if you can live with the idea of
# some data loss consider the default persistence mode that's snapshotting),
# or on the contrary, use "always" that's very slow but a bit safer than
# everysec.
#
# More details please check the following article:
# http://antirez.com/post/redis-persistence-demystified.html
#
# If unsure, use "everysec".

# appendfsync always
appendfsync everysec
# appendfsync no
```

刷盘的代码。

```c
void flushAppendOnlyFile(int force) {
...
try_fsync:
    /* Don't fsync if no-appendfsync-on-rewrite is set to yes and there are
     * children doing I/O in the background. */
    if (server.aof_no_fsync_on_rewrite && hasActiveChildProcess())
        return;

    /* Perform the fsync if needed. */
    if (server.aof_fsync == AOF_FSYNC_ALWAYS) {
        //每次都需要刷盘
        /* redis_fsync is defined as fdatasync() for Linux in order to avoid
         * flushing metadata. */
        latencyStartMonitor(latency);
        redis_fsync(server.aof_fd); /* Let's try to get this data on the disk */
        latencyEndMonitor(latency);
        latencyAddSampleIfNeeded("aof-fsync-always",latency);
        server.aof_fsync_offset = server.aof_current_size;
        server.aof_last_fsync = server.unixtime;
    } else if ((server.aof_fsync == AOF_FSYNC_EVERYSEC &&
                server.unixtime > server.aof_last_fsync)) {
        if (!sync_in_progress) {
            //每秒刷的话，如果现在没有后台任务，就加个后台任务
            aof_background_fsync(server.aof_fd);
            server.aof_fsync_offset = server.aof_current_size;
        }
        server.aof_last_fsync = server.unixtime;
    }
}
```

我们可以发送aof_rewrite请求，在执行BGSAVE的时候这个请求会被挂起，直到BGSAVE结束后才会执行。

```c
int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {
    ...
    /* Start a scheduled AOF rewrite if this was requested by the user while
     * a BGSAVE was in progress. */
    if (!hasActiveChildProcess() &&
        server.aof_rewrite_scheduled)
    {
        rewriteAppendOnlyFileBackground();
    }
    ...
}
```



# 集群

## 复制

从服务器不会删除过期的对象。只有在显式收到主服务器的del命令才会执行真正的删除。（但是请求从服务器时过期对象是无法取到的，所以和过期了是相同的结果）之所以这么做是为了维持主从服务器数据的一致性。

```c
int expireIfNeeded(redisDb *db, robj *key) {
    //未过期
    if (!keyIsExpired(db,key)) return 0;

    /* If we are running in the context of a slave, instead of
     * evicting the expired key from the database, we return ASAP:
     * the slave key expiration is controlled by the master that will
     * send us synthesized DEL operations for expired keys.
     *
     * Still we try to return the right information to the caller,
     * that is, 0 if we think the key should be still valid, 1 if
     * we think the key is expired at this time. */
    //slave过期了不执行删除，但是只返回正确信息
    if (server.masterhost != NULL) return 1;

    /* Delete the key */
    //真的删除
    server.stat_expiredkeys++;
    propagateExpire(db,key,server.lazyfree_lazy_expire);
    notifyKeyspaceEvent(NOTIFY_EXPIRED,
        "expired",key,db->id);
    //同步还是异步删除
    int retval = server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) :
                                               dbSyncDelete(db,key);
    if (retval) signalModifiedKey(NULL,db,key);
    return retval;
}

robj *lookupKeyReadWithFlags(redisDb *db, robj *key, int flags) {
    robj *val;

    if (expireIfNeeded(db,key) == 1) {
        /* Key expired. If we are in the context of a master, expireIfNeeded()
         * returns 0 only when the key does not exist at all, so it's safe
         * to return NULL ASAP. */
        if (server.masterhost == NULL) {
            //自己为主服务器
            server.stat_keyspace_misses++;
            notifyKeyspaceEvent(NOTIFY_KEY_MISS, "keymiss", key, db->id);
            return NULL;
        }

        /* However if we are in the context of a slave, expireIfNeeded() will
         * not really try to expire the key, it only returns information
         * about the "logical" status of the key: key expiring is up to the
         * master in order to have a consistent view of master's data set.
         *
         * However, if the command caller is not the master, and as additional
         * safety measure, the command invoked is a read-only command, we can
         * safely return NULL here, and provide a more consistent behavior
         * to clients accessign expired values in a read-only fashion, that
         * will say the key as non existing.
         *
         * Notably this covers GETs when slaves are used to scale reads. */
        if (server.current_client &&
            server.current_client != server.master &&
            server.current_client->cmd &&
            server.current_client->cmd->flags & CMD_READONLY)
        {
            //从服务器返回空
            server.stat_keyspace_misses++;
            notifyKeyspaceEvent(NOTIFY_KEY_MISS, "keymiss", key, db->id);
            return NULL;
        }
    }
    val = lookupKey(db,key,flags);
    if (val == NULL) {
        server.stat_keyspace_misses++;
        notifyKeyspaceEvent(NOTIFY_KEY_MISS, "keymiss", key, db->id);
    }
    else
        server.stat_keyspace_hits++;
    return val;
}
```


# 参考资料

- 《Redis设计与实现》
- [Redis内部数据结构详解(5)——quicklist](http://zhangtielei.com/posts/blog-redis-quicklist.html)