---
categories: learn-note
layout: post
---

- Table
{:toc}

# 基本流程

将训练数据(training set)喂给某个学习算法(learning algorithm)，之后算法计算出一个假说(hypothesis)，$h$。之后对于给定的输入$x$，假说会估算出来结果$h(x)$。

现在的问题是该如何表示一个假说$h$。

我们记录$n$为输入向量的长度，$m$为输出向量的长度，$k$为训练集的大小。因此训练集可以表示为$(x^{(1)},y^({1})),\ldots,(x^{(k)},y^{(k)})$。其中每个元素$(x^{(i)},y^{(i)})$叫做一个训练样本。

如果我们有多个假说，我们还需要设计一套误差计算的函数$D$，$h$在第$i$个样本上的误差为$D(h(x^{(i)})-y^{(i)})$。而在所有训练集上的总误差一般为$J(h)=\sum_{i=1}^kD(h(x^{(i)})-y^{(i)})$。这样我们就能比较两个不同的假说的优劣了。

# 线性回归(linear regression)

在线性回归中，假说$h(x)=ax+b$，即一个仿射函数。

举个例子。通过房子的信息，我们要估算房子的价格。其中$x$是一个长度为$2$的列向量$(size\_of\_house, number\_of\_bedrooms)$，而$a$则是一个$1\times 2$的矩阵，$b$是一个实数。为了让公式更加紧凑，我们可以将$x$增加一个维度，增加的维度的值始终是一个守护值$1$，这样我们就可以将假说重写为$h(x)=Ax$，一个线性函数。

线性回归算法的目标就是为你选择合适的$A$来产生尽可能精确的结果。

线性回归中，误差函数一般为$D(v)=\frac{1}{2}\|v\|^2$，即离原点的距离的平方。

## 梯度下降(gradient descent)

梯度下降是用来寻找假说中系数$A$的算法。

我们可以从任意一个随机矩阵$A$出发。之后让

$$
a_{i,j}:=a_{i,j}-\frac{\partial J}{\partial a_{i,j}}(A)\cdot J(A)\cdot \alpha$$

其中$\alpha$是一个常量，称为学习率（learning rate)，决定了学习的速率。

下面推导一下偏导数：


$$
\begin{aligned}
\frac{\partial J}{\partial a_{i,j}}(A)&=\frac{\partial}{\partial a_{i,j}}\sum_{t=1}^kD(Ax^{(t)}-y^{(t)})\\
&=\frac{1}{2}\frac{\partial}{\partial a_{i,j}}\sum_{t=1}^k(A_ix^{(t)}-y^{(t)}_i)^2\\
&=\frac{1}{2}\frac{\partial}{\partial a_{i,j}}\sum_{t=1}^k(\sum_{r}a_{i,r}x^{(t)}_r-y^{(t)}_i)^2\\
&=\frac{1}{2}\sum_{t=1}^k 2(\sum_{r}a_{i,r}x^{(t)}_r-y^{(t)}_i)x^{(t)}_j\\
&=\sum_{t=1}^k (\sum_{r}a_{i,r}x^{(t)}_r-y^{(t)}_i)x^{(t)}_j
\end{aligned}
$$

在线性回归模型中，只有唯一的极小值点，因此不会陷入局部最小值的情况。

上面的方法成为批量梯度下降(batch gradient descent)。在训练集非常大的情况下，由于批量梯度下降每一轮迭代都需要遍历整个训练集，因此会非常慢。

还有一种方法叫做随机梯度下降(stochastic gradient descent)。具体做法就是遍历整个训练集，但是对于每个单独的训练集元素都执行一轮迭代(即可以认为每次迭代的时候$k=1$)。其最后还是会停止在全局最小值处。

随机梯度下降也不是完美的，较少的错误数据或噪音可能对结果造成很大的影响。我们可以在随机梯度下降的过程中加入优化，比如每次一次性取$100$条记录来训练(每次迭代的时候$k=100$)，这样就能避免噪音带来的影响，同时训练的速度也非常快。这种方法叫做微批量梯度下降(Mini-batch gradient descent)。

除了梯度下降外，线性回归问题，我们可以不需要多次迭代，通过矩阵运算即可直接求出解。具体我们可以发现在全局最小值处，$J$的导数矩阵应该是个$0$矩阵。这时候可以发现我们可以将问题写成线性方程组，并最后通过高斯消元求解$A$。