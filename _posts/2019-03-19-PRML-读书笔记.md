---
categories: note
layout: post
---

- Table
{:toc}

# 第一章 介绍

假设我们在t=sin(x)的曲线上采样若干个点(x1,t1),(x2,t2),...,(xN,tN)作为训练集合，并向目标值加入一些满足高斯分布的噪音。现在我们希望通过多项式对原来的曲线进行拟合。

y=w0+w1\*x^1+...+wM\*x^M

上面的M是多项式的阶数。我们的目标是得到系数**w**的值。我们通过将多项式适配到训练数据来得到系数。这个过程可以通过最小化误差函数实现，误差函数用于测量函数y(x,w)与正确标签之间的不匹配度。一个简单的误差函数的选择是计算每个点上，预测值与正确值的差的平方和，即：

E(**w**)=1/2\*((y(x1,**w**)-t1)^2+...+(y(xN,**w**)-tN)^2)

我们的目标就是使上面公式值最小化，0是理想误差值，这意味在在训练数据上我们的函数y与t拥有相同的值。

由于误差函数E是w的二阶多项式，因此误差函数的导数是一阶多项式，这意味着误差函数的导数只有唯一的一个零点**w'**，而E(**w'**)一定是E所能取到的最小值，而结果多项式y(x,**w'**)就是我们要求的函数。

但是M值应该选择多少呢，过小的M值不够灵活，很可能无法达到足够的拟合。而过大的M会完全拟合训练集中的数据，但是最终得到的多项式系数过大，在对于测试集中的数据进行测试时偏差非常大，这种情况称为过拟合。

我们的目标是能得到能很好预测新输入的函数。对于选择的每一个M，得到其最优的**w'**后，我们可以将误差函数作用在测试集合上来观察y(x,**w'**)的实际效果。由于平方和与样本数量想关，因此更方便的方式是使用均方根来作为损失函数。

E_RMS=sqrt(2\*E(**w'**)/N)

这样训练集上的均方根可以用于衡量y(x,**w'**)对新数据的预测能力。

实际上，随着我们增大M，训练数据上的均方根会不断减少直到0为止，而在训练数据上的均方根会先减少后快速上升。由于阶数较大的多项式能完全包含阶数较小的多项式，因此随着阶数的上升，在训练集合上的均方根会不断减少（由牛顿插值公式可知可以得到经过所有点的多项式，因此均方根最后会降低为0）。

之所以过大的M会导致测试集合上均方根过大，是由于y(x,**w'**)的部分系数过大。但是随着训练集合的增大，过拟合问题将变得不那么严重。即，越大的训练集合，越能负担更复杂的模型。粗略推荐使用模型参数的数量的倍数大小（5或10）的训练集合。但是训练集合的大小并一定是模型复杂度最好的衡量方式。

并且按照可用训练集的大小来限定模型参数的数目并不让人满意。看起来按照要解决的问题的复杂度来选择模型的复杂度是更加可信的方式，

我们将看到最小平方的方式寻找参数模型，实际是最大似然估计的一种特殊情况，而过拟合问题则可以被理解为最大似然估计的一个通常属性。通过采用贝叶斯方法，可以避免过拟合问题。我们将看到可以通过贝叶斯视角毫无难度地采用一个参数数目远超训练集大小的模型。实际上，在贝叶斯模型中，有效 参数数目会自动适应训练集的大小。

目前，用现在的方法继续并思考如何在实践中在较小的训练集合上采用更加灵活复杂的模型，是非常有益处的。正则化是一种经常用来在这种情况下控制过拟合现象的技术。它通过向误差函数追加一项惩罚项，来阻碍模型的参数值过大。最简单的惩罚项，是取所有参数的平方和。这样导出新的误差函数：

E'(**w**)=E(**w**)+a/2\***w**\***w**^T

其中系数a用于调整正则项对结果影响的强度。一般w0不会算在正则化子中，因为如果加入w0，会导致误差依赖于选择的模型在x=0时所取的值，或则可以加入w0，但是要分配一个独立的系数。在统计学中，这样的技术称为收缩方法，因为他们会减少系数的值。上面的二阶正则化子的特殊情况称为脊回归（ridge regression）。在神经网络上下文中，这种方法也称为权重衰退（weight decay）。

过小的调整因子会导致最终模型系数过大，而过大的调整因子会使得最终的模型趋向于直线，缺乏预测能力。只有合适的调整因子才可以真正得到合适的模型。

可以将训练集合切分为两部分，第一部分为训练集合，第二部分为验证集合。前者用于得到最优的系数**w'**，而后者则用于优化模型的复杂度（比如M和a）。但是在许多情况，会被证明过于浪费珍贵的训练数据，因此我们需要找到更加复杂度的方式。