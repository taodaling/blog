---
categories: notebook
layout: post
---

- Table
{:toc}

# 简介

机器学习是指给定目标，程序通过从经验中学习，最后提高它的性能指标。

根据目标的取值范围，我们可以将问题分类为：

- 回归问题：连续的值，比如所有的实数
- 分类问题：输出离散的值，比如0,1,2

根据我们是否对样本进行标记，机器学习分为：

- 监督学习：对样本进行标记，比如图像识别
- 无监督学习：没有对样本进行标记，比如聚类问题。

# 监督学习

监督学习的基本流程为：

给定训练集合$x, y$，其中$x$是输入，$y$是正确的输出，记$x^{(i)},y^{(i)}$表示第$i$个样本的输入和输出，记$m$为样本数量，$n$为特征数量。

之后我们需要通过训练集合训练出一个假说函数$h$，这个假说函数拥有最小的惩罚，其中惩罚为$J(h)$。这里我们认为$h$由参数向量$\theta$确定。

最后我们利用$h$预测新数据的输出。

# 多元线性回归

线性回归算法的假说函数为$h(x)=\theta_1^Tx+\theta_0$，其中$\theta_1$是一个长度为$n$的向量，$\theta_0$为常数。或者我们可以为每个$x$后面补充一个$1$，这时候可以简化为$h(x)=\theta^T x$。

我们的目标是最小化下面的惩罚函数$J$：

$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2
$$

计算导数如下：

$$
\begin{aligned}
\frac{\partial}{\partial \theta_j}J(\theta)&=\frac{1}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})x^{(i)}_j
\end{aligned}
$$

线性回归模型的性质：

- 只有唯一的极值点，同时这个点也是全局极小值点。（因此很适合使用梯度下降算法）

# 逻辑回归

逻辑回归用于解决分类问题，其假说函数为：

$$
h(x)=\mathrm{sigmoid}(\theta^Tx)=\frac{1}{1+e^{-\theta^T x}}
$$

这里顺带一提，$\mathrm{sigmoid}$函数的导数特别简单，$\mathrm{sigmoid}'(z)=\mathrm{sigmoid}(z)(1-\mathrm{sigmoid}(z))$。

其中$h(x)$可以理解为对于输入$x$，其输出为$1$的概率，即$h(x)=P(y=1\mid x;\theta)$。

考虑到$h$产生的是$\[0,1\]$之间的连续值，但是分类问题需要的是离散值，因此我们可以在$h(x)\geq 0.5$的时候输出$1$，否则输出$0$（输出较大可能的项）。考虑到$\mathrm{sigmoid}$函数的特性，可以发现输出$1$当且仅当$\theta^Tx\geq 0$。再考虑到线性函数的模型，其等高线$\theta^Tx=0$对应一个非规则轮廓，如果$x$落在非规则轮廓内部输出$0$，外部输出$1$，这条等高线称为决策边界。

我们不能简单的使用$J(\theta)=\frac{1}{2m}\|h(x)-y\|^2$作为惩罚函数，因为此时函数可能有多个局部极值点，不是凸函数。

考虑到$y\in\left\\{0,1\right\\}$，因此我们可以得出

$$
P(y|x;\theta)=h(x)^y(1-h(x))^{1-y}
$$

根据最大似然估计，得出：

$$
\begin{aligned}
l(\theta)&=P(y|x;\theta)\\
&=\prod_{i=1}^mP(y^{(i)}|x^{(i)};\theta)\\
&=\prod_{i=1}^mh(x^{(i)})^{y^{(i)}}(1-h(x^{(i)}))^{1-y^{(i)}}
\end{aligned}
$$

其对数形式为：

$$
\begin{aligned}
\ln l(\theta) &= \ln \prod_{i=1}^mh(x^{(i)})^{y^{(i)}}(1-h(x^{(i)}))^{1-y^{(i)}}\\
&= \sum_{i=1}^m y^{(i)}\ln h(x^{(i)})+(1-y^{(i)})\ln(1-h(x^{(i)}))
\end{aligned}
$$

我们希望最大化$l(\theta)$，等价于最大化$\ln l(\theta)$。

令$J(\theta)=-\frac{1}{m}\ln l(\theta)$，此时的惩罚函数为凸函数，可以利用梯度下降算法进行求解，其偏导数为：

$$
\begin{aligned}
\frac{\partial}{\partial \theta_j}J(\theta)&=\frac{1}{m}\sum_{i=1}^m (h(x^{(i)})-y^{(i)})x^{(i)}_j
\end{aligned}
$$

# 多元逻辑回归

我们已经知道逻辑回归可以处理$y$的值仅两种可能的情况，下面我们设计一个算法，可以将输入分类到$k$个类别中。具体做法就是，我们通过逻辑回归得到$k$个模型，其中$h^{(i)}(x)$表示输入为第$i$个类别元素的概率。之后我们选择$\mathrm{argmax}_i h^{(i)}(x)$作为输出，即最可能的类型。

# 梯度下降算法

我们可以把惩罚函数$J$绘制成图形。

![https://raw.githubusercontent.com/taodaling/taodaling.github.io/master/assets/images/machine-learning/gradient-descending.png](https://raw.githubusercontent.com/taodaling/taodaling.github.io/master/assets/images/machine-learning/gradient-descending.png)

梯度下降算法实际上是选择一条下坡的道路（沿梯度的逆向）。当不存在这样的道路的时候，就意味着我们到了一个局部极小值点。

梯度下降分为多次迭代，每次迭代我们都通过下面方式修正参数$\theta$。

$$
\theta := \theta-\alpha \frac{\mathrm{d}}{\mathrm{d} \theta}J(\theta)
$$

其中$\alpha$是一个称为学习率的参数。

梯度下降法的性质：

- 如果迭代点抵达某个极值点，那么迭代点不会再发生变化。
- $\theta$越是接近极值点，梯度下降的步长就会越小，因此不必特意不断减少$\alpha$。

## 变种

上面的方法称为批量梯度下降(batch gradient descent)。在训练集非常大的情况下，由于批量梯度下降每一轮迭代都需要遍历整个训练集，因此会非常慢。

还有一种方法叫做随机梯度下降(stochastic gradient descent)。具体做法就是遍历整个训练集，但是对于每个单独的训练集元素都执行一轮迭代(即可以认为每次迭代的时候$m=1$)。其最后还是会停止在全局最小值处。

随机梯度下降也不是完美的，较少的错误数据或噪音可能对结果造成很大的影响。我们可以在随机梯度下降的过程中加入优化，比如每次一次性取$100$条记录来训练(每次迭代的时候$m=100$)，这样就能避免噪音带来的影响，同时训练的速度也非常快。这种方法叫做微批量梯度下降(Mini-batch gradient descent)。

# 特征缩放

如果某个特征的范围特别大，那么等高线会变得很狭长，这样会导致梯度下降算法需要更多的迭代次数。

特征缩放是指将第$i$个特征归约为$x_i:=\frac{x_i-\mu_i}{s_i}$，其中$\mu_i$为第$i$个特征的平均值，而$s_i$是第$i$个特征的范围（最大值减去最小值）。可以发现这样归约后有$-2\leq x_i\leq 2$。

# 学习率的选择

只要学习率足够小，就能保证每一步的迭代都会减少$J(\theta)$。但是过小的学习率会导致收敛变慢。

# 正规方程

对于多元线性回归模型，可以使用正规方程一步计算到位结果。

我们可以假设所有特征之间线性无关（否则可以不断删除冗余的列，直到所有列线性无关），那么可以直接用投影矩阵找到参数$\theta$。以样本向量作为行向量构成$m\times n$的矩阵$X$，可以直接得到：

$$
\theta=(X^TX)^{-1}X^Ty
$$

正规方程的解法时间复杂度为$O((m+n)n^2)$，因此在特征数$n$很大的时候会运行的很缓慢。同时如果样本集很大，也很难将所有的样本同时加载到内存中做矩阵运算。

这里特殊提一下，如果$X$的列向量线性相关，我们也可以利用伪逆进行求解。记$X^+$为$X$的伪逆，我们可以令$\theta=X^+y$。

# 正则化

当我们的特征数过多，容易得到一个误差接近$0$的模型，但是这时候样本数不足以约束模型，因此这时候模型无法泛化到新的样本中。这时候我们称模型过拟合。

当我们的特征数不足，这时候会得到一个误差很大的模型。这时候称模型欠拟合。

对于惩罚函数$J(h)$，我们实际上是加上一段与模型相关的乘法，此时修正后的惩罚函数为$J(h)+\lambda\mathrm{Reg}(h)$。其中$\lambda$表示正则化的强度，$\lambda$越大，则对模型约束越大，防止过拟合效果越好，但是对应的也会导致模型在训练数据上效果的变差，即可能导致欠拟合。

对于线性模型，$h$由向量$\theta$所决定，我们可以定义$\mathrm{Reg}(\theta)=\frac{1}{2m}\theta^T\theta$，这样可以保证训练出来的模型参数比较小，从而保证模型比较简单，不容易出现过拟合。这里特殊提一下，一般我们在计算$\mathrm{Reg}(\theta)$的时候是不会考虑之前插入的全$1$的特征的。

# 神经网络

在逻辑回归的时候，当特征数量很大的时候，我们很难选择特征，并且生成新的特征。这时候就需要用到神经网络了。

神经网络分成多个层次，每个层次都包含多个神经元，第一层称为输入层，最后一层称为输出层，中间的其余层称为隐藏层。可以将神经元理解为一个sigmod函数$g$，接受输入，并提供输出（每一层增加一个隐藏的神经元，始终输出常数$1$），而神经元之间的线路对应一个常数，这个常数对这个神经元的输出进行缩放。在神经网络中，输入经过输入层、隐藏层、输出层的变换，最后输出实数的这个过程称为前向传播。

![https://upload.wikimedia.org/wikipedia/commons/9/97/Ncell.png](https://upload.wikimedia.org/wikipedia/commons/9/97/Ncell.png)

记录$L$为神经网络层数，记录$s_i$表示第$i$层神经元个数，记录$a_i^{(j)}$表示第$j$层的第$i$个神经元的输出，$\Theta^{(i)}$是一个$s_{i+1}\times s_{i}$的矩阵，矩阵的第$k$列对应第$i$层输出到$a_{k}^{(i+1)}$的模型参数，$\Theta^{(i)}$称为权重矩阵，表示从第$i$层向第$i+1$层的转换。经过$\Theta^{(i)}$转换后，第$i+1$层得到的输入为$z^{(i+1)}=\Theta^{(i)}a^{(i)}$，满足$g(z^{(i)})=a^{(i)}$。记录$K$为输出层的神经元数目（即$s_L$）。记录$\delta^{(l)}_i$表示第$l$层第$i$个神经元的误差。

如果希望神经网络做多分类，可以在输出层设置多个神经元，这样神经网络的输出就是一个向量了，向量的第$i$个元素表示输入是否属于类别$i$。

神经网络的惩罚函数定义为:

$$
\begin{aligned}
J(\Theta)=&-\frac{1}{m}\left[\sum_{i=1}^m\sum_{k=1}^K y_k^{(i)}\ln h(x^{(i)})_k+(1-y_k^{(i)})\ln(1-h(x^{(i)})_k)\right]\\&+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta^{(l)}_{j,i})^2
\end{aligned}
$$

在做正则化的时候我们同样不考虑那些增加的常数$1$神经元的输出的参数。

偏导数可以通过链式求导的方式计算：

下面的计算我们仅考虑一个测试用例，多个测试用例只需要计算出单个测试用例的部分，之后加总即可。同时我们忽略系数$\frac{1}{m}$和正则化项的影响。定义$\delta^{(l)}$表示$\frac{\mathrm{d}J}{\mathrm{d}a^{(l)}}$，可以发现有$\delta^{(l)}=a^{(L)}-y$。对于其余项：

$$
\begin{aligned}
\delta^{(l)}&=\frac{\mathrm{d}J}{\mathrm{d}a^{(l)}}\\
&=\frac{\mathrm{d}J}{\mathrm{d}a^{(l+1)}}\frac{\mathrm{d}a^{(l+1)}}{\mathrm{d}z^{(l+1)}}\frac{\mathrm{d}z^{(l+1)}}{\mathrm{d}a^{(l)}}\\
&=\delta^{(l+1)}\mathrm{diag}(a^{(l+1)}.*(1-a^{(l+1)}))\Theta^{(l)}\\
&=(\delta^{(l+1)}.*a^{(l+1)}.*(1-a^{(l+1)}))\Theta^{(l)}
\end{aligned}
$$

其中$\mathrm{diag}(A)$表示把向量$A$作为对角线元素组成一个新的方阵，而$A.\*B$表示按位置乘，即$(A.\*B)_{i,j}=A_{i,j}B_{i,j}$。

接下来定义$\Delta^{(l)}_{i,j}=\frac{\mathrm{d}J}{\mathrm{d}\Theta^{(l)}_{i,j}}$。可以发现

$$
\begin{aligned}
\Delta^{(l)}_{i,j}&=\frac{\mathrm{d}J}{\mathrm{d}\Theta^{(l)}_{i,j}}\\
&=\frac{\mathrm{d}J}{\mathrm{d}a^{(l+1)}}\frac{\mathrm{d}a^{(l+1)}}{\mathrm{d}z^{(l+1)}}\frac{\mathrm{d}z^{(l+1)}}{\mathrm{d}\Theta^{(l)}_{i,j}}\\
&=\delta^{(l+1)}\mathrm{diag}(a^{(l+1)}.*(1-a^{(l+1)}))[i:a_j^{(l)}]\\
&=(\delta^{(l+1)}.*a^{(l+1)}.*(1-a^{(l+1)}))_i\cdot a_j^{(l)}
\end{aligned}
$$

其中$\[i:a_j^{(l)}\]$是个向量，只有第$i$行一个非零值$a_j^{(l)}$。

最后加入我们久违的系数和正则化项后，得到：

$$
\frac{\mathrm{d}J}{\mathrm{d}\Theta^{(l)}_{i,j}}=
\left\{
\begin{array}{ll}
\frac{1}{m}\Delta^{(l)}_{i,j} &\text{if }j=0\\
\frac{1}{m}\Delta^{(l)}_{i,j}+\frac{\lambda}{m}\Theta^{(l)}_{i,j} &\text{otherwise}
\end{array}
\right.
$$

神经网络的惩罚函数并不是一个凸函数，因此可能存在多个局部极值点。

## 随机初始化

如果初始化$\Theta$的时候选择$0$矩阵，会导致每一层的神经元的输出都相同，对应的它们的偏导数也会相同。这样无论多少次迭代后，每一层的神经元只有一个输出，十分浪费。

一个较好的方法时，用$(-\epsilon, \epsilon)$之间的随机数填充$\Theta$，注意$\epsilon$不能太大，可以取$1$。

# 梯度检测

如果写梯度下降算法的时候，偏导数计算错误，可能也会得到一个使得惩罚下降的结果，但是效果会相差十万八千里。

那么该如何发现偏导数计算上的错误呢。

一种简单的方法就是计算两侧偏导。

$$
\frac{\partial}{\partial \theta_i}J(\theta)\approx \frac{J(\theta+\epsilon)-J(\theta-\epsilon))}{2\epsilon}
$$

求出两侧偏导后，对照计算出来的导数看看结果是否相近，这里可以取$\epsilon=10^{-4}$或者其它较小的数。

