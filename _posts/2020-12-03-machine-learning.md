---
categories: notebook
layout: post
---

- Table
{:toc}

# 简介

机器学习是指给定目标，程序通过从经验中学习，最后提高它的性能指标。

根据目标的取值范围，我们可以将问题分类为：

- 回归问题：连续的值，比如所有的实数
- 分类问题：输出离散的值，比如0,1,2

根据我们是否对样本进行标记，机器学习分为：

- 监督学习：对样本进行标记，比如图像识别
- 无监督学习：没有对样本进行标记，比如聚类问题。

# 监督学习

监督学习的基本流程为：

给定训练集合$x, y$，其中$x$是输入，$y$是正确的输出，记$x^{(i)},y^{(i)}$表示第$i$个样本的输入和输出，记$m$为样本数量，$n$为特征数量。

之后我们需要通过训练集合训练出一个假说函数$h$，这个假说函数拥有最小的惩罚，其中惩罚为$J(h)$。这里我们认为$h$由参数向量$\theta$确定。

最后我们利用$h$预测新数据的输出。

# 多元线性回归

线性回归算法的假说函数为$h(x)=\theta_1^Tx+\theta_0$，其中$\theta_1$是一个长度为$n$的向量，$\theta_0$为常数。或者我们可以为每个$x$后面补充一个$1$，这时候可以简化为$h(x)=\theta^T x$。

我们的目标是最小化下面的惩罚函数$J$：

$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2
$$

计算导数如下：

$$
\begin{aligned}
\frac{\partial}{\partial \theta_j}J(\theta)&=\frac{1}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})x^{(i)}_j
\end{aligned}
$$

线性回归模型的性质：

- 只有唯一的极值点，同时这个点也是全局极小值点。（因此很适合使用梯度下降算法）

# 逻辑回归

逻辑回归用于解决分类问题，其假说函数为：

$$
h(x)=\mathrm{sigmoid}(\theta^Tx)=\frac{1}{1+e^{-\theta^T x}}
$$

其中$h(x)$可以理解为对于输入$x$，其输出为$1$的概率，即$h(x)=P(y=1\mid x;\theta)$。

考虑到$h$产生的是$\[0,1\]$之间的连续值，但是分类问题需要的是离散值，因此我们可以在$h(x)\geq 0.5$的时候输出$1$，否则输出$0$（输出较大可能的项）。考虑到$\mathrm{sigmoid}$函数的特性，可以发现输出$1$当且仅当$\theta^Tx\geq 0$。再考虑到线性函数的模型，其等高线$\theta^Tx=0$对应一个非规则轮廓，如果$x$落在非规则轮廓内部输出$0$，外部输出$1$，这条等高线称为决策边界。

我们不能简单的使用$J(\theta)=\frac{1}{2m}\|h(x)-y\|^2$作为惩罚函数，因为此时函数可能有多个局部极值点，不是凸函数。

考虑到$y\in\left\\{0,1\right\\}$，因此我们可以得出

$$
P(y|x;\theta)=h(x)^y(1-h(x))^{1-y}
$$

根据最大似然估计，得出：

$$
\begin{aligned}
l(\theta)&=P(y|x;\theta)\\
&=\prod_{i=1}^mP(y^{(i)}|x^{(i)};\theta)\\
&=\prod_{i=1}^mh(x^{(i)})^{y^{(i)}}(1-h(x^{(i)}))^{1-y^{(i)}}
\end{aligned}
$$

其对数形式为：

$$
\begin{aligned}
\ln l(\theta) &= \ln \prod_{i=1}^mh(x^{(i)})^{y^{(i)}}(1-h(x^{(i)}))^{1-y^{(i)}}\\
&= \sum_{i=1}^m y^{(i)}\ln h(x^{(i)})+(1-y^{(i)})\ln(1-h(x^{(i)}))
\end{aligned}
$$

我们希望最大化$l(\theta)$，等价于最大化$\ln l(\theta)$。

令$J(\theta)=-\frac{1}{m}\ln l(\theta)$，此时的惩罚函数为凸函数，可以利用梯度下降算法进行求解，其偏导数为：

$$
\begin{aligned}
\frac{\partial}{\partial \theta_j}J(\theta)&=\frac{1}{m}\sum_{i=1}^m (h(x^{(i)})-y^{(i)})x^{(i)}_j
\end{aligned}
$$

# 多元逻辑回归

我们已经知道逻辑回归可以处理$y$的值仅两种可能的情况，下面我们设计一个算法，可以将输入分类到$k$个类别中。具体做法就是，我们通过逻辑回归得到$k$个模型，其中$h^{(i)}(x)$表示输入为第$i$个类别元素的概率。之后我们选择$\mathrm{argmax}_i h^{(i)}(x)$作为输出，即最可能的类型。

# 梯度下降算法

我们可以把惩罚函数$J$绘制成图形。

![https://raw.githubusercontent.com/taodaling/taodaling.github.io/master/assets/images/machine-learning/gradient-descending.png](https://raw.githubusercontent.com/taodaling/taodaling.github.io/master/assets/images/machine-learning/gradient-descending.png)

梯度下降算法实际上是选择一条下坡的道路（沿梯度的逆向）。当不存在这样的道路的时候，就意味着我们到了一个局部极小值点。

梯度下降分为多次迭代，每次迭代我们都通过下面方式修正参数$\theta$。

$$
\theta := \theta-\alpha \frac{\mathrm{d}}{\mathrm{d} \theta}J(\theta)
$$

其中$\alpha$是一个称为学习率的参数。

梯度下降法的性质：

- 如果迭代点抵达某个极值点，那么迭代点不会再发生变化。
- $\theta$越是接近极值点，梯度下降的步长就会越小，因此不必特意不断减少$\alpha$。

## 变种

上面的方法称为批量梯度下降(batch gradient descent)。在训练集非常大的情况下，由于批量梯度下降每一轮迭代都需要遍历整个训练集，因此会非常慢。

还有一种方法叫做随机梯度下降(stochastic gradient descent)。具体做法就是遍历整个训练集，但是对于每个单独的训练集元素都执行一轮迭代(即可以认为每次迭代的时候$m=1$)。其最后还是会停止在全局最小值处。

随机梯度下降也不是完美的，较少的错误数据或噪音可能对结果造成很大的影响。我们可以在随机梯度下降的过程中加入优化，比如每次一次性取$100$条记录来训练(每次迭代的时候$m=100$)，这样就能避免噪音带来的影响，同时训练的速度也非常快。这种方法叫做微批量梯度下降(Mini-batch gradient descent)。

# 特征缩放

如果某个特征的范围特别大，那么等高线会变得很狭长，这样会导致梯度下降算法需要更多的迭代次数。

特征缩放是指将第$i$个特征归约为$x_i:=\frac{x_i-\mu_i}{s_i}$，其中$\mu_i$为第$i$个特征的平均值，而$s_i$是第$i$个特征的范围（最大值减去最小值）。可以发现这样归约后有$-2\leq x_i\leq 2$。

# 学习率的选择

只要学习率足够小，就能保证每一步的迭代都会减少$J(\theta)$。但是过小的学习率会导致收敛变慢。

# 正规方程

对于多元线性回归模型，可以使用正规方程一步计算到位结果。

我们可以假设所有特征之间线性无关（否则可以不断删除冗余的列，直到所有列线性无关），那么可以直接用投影矩阵找到参数$\theta$。以样本向量作为行向量构成$m\times n$的矩阵$X$，可以直接得到：

$$
\theta=(X^TX)^{-1}X^Ty
$$

正规方程的解法时间复杂度为$O((m+n)n^2)$，因此在特征数$n$很大的时候会运行的很缓慢。同时如果样本集很大，也很难将所有的样本同时加载到内存中做矩阵运算。

这里特殊提一下，如果$X$的列向量线性相关，我们也可以利用伪逆进行求解。记$X^+$为$X$的伪逆，我们可以令$\theta=X^+y$。

# 正则化

目前还没具体了解原理，据说是加上一些先验条件，防止过拟合。

对于惩罚函数$J(h)$，我们实际上是加上一段与模型相关的乘法，此时修正后的惩罚函数为$J(h)+\lambda\mathrm{Reg}(h)$。其中$\lambda$表示正则化的强度，$\lambda$越大，则对模型约束越大，防止过拟合效果越好，但是对应的也会导致模型在训练数据上效果的变差。

对于线性模型，$h$由向量$\theta$所决定，我们可以定义$\mathrm{Reg}(\theta)=\frac{1}{2m}\theta^T\theta$，这样可以防止模型为了一些很小的特征，而将对应该特征的参数过分放大。

