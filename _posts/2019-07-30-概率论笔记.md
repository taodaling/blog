---
categories: mathematic
layout: post
---

- Table
{:toc}

# 单调函数密度函数

已知随机变量$X$的概率密度函数$f_X(x)$，定义$Y=g(X)$，$g$是严格单调函数，我们可以用下面的方法计算$f_Y(y)$。

首先记$h=g^{-1}$。我们计算$Y$的分布函数。仅考虑递增的情况。

$$
F_Y(y)=P(Y\leq y) =P(g(X)\leq  y)=P(X\leq h(y))=F_X(h(y))
$$

接下来对分布函数进行求导，得到概率密度函数：

$$
f_Y(y)=\frac{\partial F_Y(y)}{\partial y}=\frac{\partial F_X(h(y))}{\partial y}=f_X(h(y))h'(y)=f_X(h(y))|h'(y)|
$$

# 绝对值密度函数

记$Y=\|X\|$，计算，计算$F_Y$

如下：

$$
F_Y(y)=P(Y\leq y) = P(|X|\leq y)=P(-y\leq X\leq y)=F_X(y)-F_X(-y)
$$

对分布函数求导得到密度函数：

$$
f_Y(y)=\frac{\partial F_Y(y)}{\partial y}=f_X(y)+f_X(-y)
$$

# 重期望法则

对于任意两个随机变量X、Y。

$$
E[X]=\int_{-\infty}^{\infty}xf_X(x)dx\\
=\int_{-\infty}^{\infty}xdx\int_{-\infty}^{\infty} f_{X|Y}(x|y)f_Y(y)dy\\
=\int_{-\infty}^{\infty}f_Y(y)dy\int_{-\infty}^{\infty} xf_{X|Y}(x|y)dx\\
=\int_{-\infty}^{\infty}E[X|Y=y]f_Y(y)dy\\
=E[E[X|Y]]
$$

离散情况下只需要将积分符号替换为$\sum$即可。

# 重方差法则

对于任意两个随机变量X、Y。

$$
var[X]=E[X^2]-E[X]^2\\
=E[E[X^2|Y]]-E[E[X|Y]]^2\\
=E[E[X|Y]^2]-E[E[X|Y]]^2+E[E[X^2|Y]]-E[E[X|Y]^2]\\
=var[E[X|Y]]+E[E[X^2|Y]-E[X|Y]^2]\\
=var[E[X|Y]]+E[var[X|Y]]
$$

# 矩母函数

随机变量X的矩母函数定义为：

$$
E_X(s)=E[e^{sX}]
$$

对矩母函数进行求导可以得到：

$$
\frac{d}{ds^n}M_X(s)=\frac{d}{ds^n}\int_{-\infty}^{\infty} e^{sx}f_X(x)dx\\
=\int_{-\infty}^{\infty} \frac{d(e^{sx})}{ds^n}f_X(x)dx\\
=\int_{-\infty}^{\infty} x^ne^{sx}f_X(x)dx
$$

当s取0时有：

$$
\frac{d}{ds^n}M_X(0)=\int_{-\infty}^{\infty} x^nf_X(x)dx=E[X^n]
$$

# 两个变量的和

记$Z=X+Y$，计算分布函数得到：

$$
F_{Z|X}(z|x)=P(Z\leq z | X= x)=P(X+Y\leq z | X = x)\\
=P(Y\leq z-x)=F_Y(z-x)
$$

对分布函数求导得到密度函数：

$$
f_{Z|X}(z|x)=\frac{d}{dz}F_{Z|X}(z|x)=\frac{d}{dz}F_Y(z-x)=f_Y(z-x)
$$

同理，当$X=Z-Y$时，可以推出

$$
F_{X|Y}(x|y)=F_Z(y+x)\\
f_{X|Y}(x|y)=f_Z(y+x)
$$

# 协方差和相关性

定义协方差$cov(X,Y)$：

$$
cov(X,Y)=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]
$$


定义相关系数$\rho(X,Y)$：

$$
\rho(X,Y)=\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}
$$

可以证明施瓦兹不等式：

$$
0\leq E[(X-\frac{E[XY]}{E[Y^2]}Y)^2]\\
=E[X^2-2\frac{E[XY]}{E[Y^2]}XY+\frac{(E[XY])^2}{(E[Y^2])^2}Y^2]\\
=E[X^2]-2\frac{E[XY]}{E[Y^2]}E[XY]+\frac{(E[XY])^2}{(E[Y^2])^2}E[Y^2]\\
=E[X^2]-\frac{(E[XY])^2}{E[Y^2]}\\
\Rightarrow E[X^2]E[Y^2]\geq (E[XY])^2
$$

当$E\[Y^2\]=0$时，最终得到的命题也是有效的。

当随机变量$X$与$Y$的相关系数有定义时（$var(X),var(Y)>0$），记$\overline{X}=X-E\[X\]，\overline{Y}=Y-E\[Y\]$，利用施瓦兹不等式可以得到：

$$
\rho(X,Y)^2=\frac{E(\overline{X}\overline{Y})^2}{E[\overline{X}^2]E[\overline{Y}^2]}\leq 1\\
\Rightarrow -1\leq \rho(X,Y) \leq 1
$$

下面证明$\|\rho(X,Y)\|=1$当且仅当$\overline{Y}=k\overline{X}的概率为1$，其中$k$是常数。先证明必要性：

$$
\rho(X,Y)=\frac{E[k\overline{X}^2]}{\sqrt{k^2E[\overline{X}]^2}}=\frac{k}{|k|}
$$

再证明充分性，利用施瓦兹不等式可以得到等号成立的必要条件：

$$
E[\overline{X}^2]E[\overline{Y}^2]= (E[\overline{X}\overline{Y}])^2\\
\Rightarrow \overline{X}=\frac{E[\overline{X}\overline{Y}]}{E[\overline{Y}^2]}\overline{Y}=\sqrt{\frac{E[\overline{X}^2]}{E[\overline{Y}^2]}}\rho(X,Y)\overline{Y}
$$

# 马尔可夫链队列问题

队列长度为n，一旦队列满了，新来的信号就会被丢弃。队列被消费的时间遵循强度为$\lambda$的指数分布，队列到达信号的时间遵循强度为$\mu$的指数分布。

我们可以建立$n+1$个状态，第$i$个状态对应队列包含$i$个信号。

对于状态i的稳态概率为$\pi_i$。

可以推出：

$$
\pi_0=(1-\mu)\pi_0+\lambda \pi_1\\
\Rightarrow \pi_1=\frac{\mu}{\lambda}\pi_0
$$

考虑对于任意$i\geq 1$，有：

$$
\pi_i=\mu\pi_{i-1}+(1-\mu-\lambda)\pi_i+\lambda\pi_{i+1}\\
\Rightarrow (\mu+\lambda)\pi_i=\lambda\pi_{i}+\lambda\pi_{i+1}\\
\Rightarrow\pi_{i+1}=\frac{\mu}{\lambda}\pi_{i}
$$

容易发现$\pi_i$是等差数列，可以直接得到$\pi_i=(\frac{\mu}{\lambda})^i\pi_0$。利用稳态概率之和为1继续推导：

$$
\pi_0+\pi_1+\ldots+\pi_n=1\\
\Rightarrow \sum_{i=0}^n\pi_0(\frac{\mu}{\lambda})^i=1\\
\Rightarrow \pi_0=\frac{(\frac{\mu}{\lambda})^{n+1}-1}{\frac{\mu}{\lambda}-1}\\
\Rightarrow \pi_i=\frac{(\frac{\mu}{\lambda})^{n+1}-1}{\frac{\mu}{\lambda}-1}
\cdot (\frac{\mu}{\lambda})^i 
$$


再考虑期望：

$$
E[X]=\sum_{i=0}^ni\pi_i=\pi_0\sum_{i=0}^ni(\frac{\mu}{\lambda})^i\\
\Rightarrow \pi_0\cdot \frac{n(\frac{\mu}{\lambda})^{n+2}-(n+1)(\frac{\mu}{\lambda})^{n+1}+(\frac{\mu}{\lambda})}{(\frac{\mu}{\lambda}-1)^2}
$$

# 线性最小均方估计

设$\Theta$是均值为$\mu$
方差为$\sigma_0^2$
的随机变量，$X_1,\ldots,X_n$是一下形式的多个观测值：$X_i=\Theta+W_i$。其中观测误差$W_i$的均值为0，方差为$\sigma_i^2$。并且假设$\Theta,W_1,\ldots, W_n$是互不相关的。现在我们通过选择系数使得下面函数取到最小值：

$$
h(a_1,\ldots,a_n,b)=\frac{1}{2}E[(\Theta-\sum_{i=1}^na_iX_i-b)^2]
$$

而最优的取值方案为：

$$
\hat\Theta=\frac{\mu/\sigma_0^2+\sum_{i=1}^nX_i/\sigma_i^2}{\sum_{i=1}^n1/\sigma_i^2}
$$

证明: 我们需要证明当均方取到最小值时
$$
a_1,\ldots, a_n,b
$$
是：

$$
b^*=\frac{\mu/\sigma_0^2}{\sum_{i=1}^n1/\sigma_i^2}\\
a_j^*=\frac{1/\sigma_i^2}{\sum_{i=1}^n1/\sigma_i^2}
$$

换言之，此时均方差的对于$a_1,\ldots, a_n,b$的偏导数均为0。求导得到

$$
\frac{\partial h}{\partial b}=E[(\sum_{i=1}^na_i^*-1)\Theta+\sum_{i=1}^na_i^*W_i+b^*]\\
\frac{\partial h}{\partial a_i}=E[X_i((\sum_{i=1}^na_i^*-1)\Theta+\sum_{i=1}^na_i^*W_i+b^*)]\\
$$

根据$b^*,a_i^*$
的定义可以得到

$$
\sum_{i=1}^na_i^*=1-\frac{b^*}{\mu}
$$

替换得到：

$$
\frac{\partial h}{\partial b}=E[-\frac{b^*}{\mu}\Theta+\sum_{i=1}^na_i^*W_i+b^*]=0
$$

接着处理$\frac{\partial h}{\partial a_i}$部分：

$$
\frac{\partial h}{\partial a_i}\\
=E[X_i((-\frac{b^*}{\mu})\Theta+\sum_{i=1}^na_i^*W_i+b^*)]\\
=E[X_i((\mu - \Theta)\frac{b^*}{\mu}+\sum_{i=1}^na_i^*W_i)]\\
=E[(\Theta+W_i)((\mu - \Theta)\frac{b^*}{\mu}+\sum_{i=1}^na_i^*W_i)]\\
=a_i^*\sigma_i^2-\sigma_0^2\frac{b^*}{\mu}\\
=0
$$

至此，证明完毕。
